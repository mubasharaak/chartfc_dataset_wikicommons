{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18dd0a52",
   "metadata": {},
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "406f027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages \n",
    "\n",
    "import bs4 \n",
    "import boto3\n",
    "import botocore\n",
    "import botocore.exceptions\n",
    "import copy\n",
    "import dns\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import random\n",
    "import requests\n",
    "import uuid\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from pymongo import MongoClient, InsertOne\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5414f1",
   "metadata": {},
   "source": [
    "## Overview \n",
    "This notebook executes the crowdsourcing experiments for the three different tasks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103ec833",
   "metadata": {},
   "source": [
    "1. <strong>Generate tasksets </strong>\n",
    "\n",
    "\n",
    "2. <strong>Label gold tasks </strong>\n",
    "\n",
    "\n",
    "3. <strong>Sent tasks to Mechanical Turk for annotation</strong> \n",
    "\n",
    "\n",
    "4. <strong>Frequently update task with workers' answers</strong> \n",
    "\n",
    "Set the following constants first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c8706307",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = r\"C:\\Users\\k20116188\\PycharmProjects\\chartfc_dataset_wikicommons\"\n",
    "\n",
    "# Folder containing all data files for crowdsourcing\n",
    "DATA_FOLDER = os.path.join(PROJECT_PATH, 'data')\n",
    "\n",
    "# Folder containing all config files for crowdsourcing\n",
    "CONFIG_FOLDER = os.path.join(PROJECT_PATH, 'config')\n",
    "TASK_CONFIG = os.path.join(CONFIG_FOLDER, 'task_config_{}.json') \n",
    "\n",
    "# Set path to .json file with crowdsourcing qualification tests\n",
    "PATH_QUALIFICATION_TESTS = os.path.join(PROJECT_PATH, 'data\\mturk\\qualification_tests.json')\n",
    "\n",
    "# Set task type as one of the following: 'table_annotation', 'claim_generation', 'adjusted_claim_annotation'\n",
    "TASK_TYPE_LIST = [\"chart_filtering\", \"claim_generation\", \"claim_verification\", \"explanation_verification\"]\n",
    "TASK_TYPE = TASK_TYPE_LIST[0] # selected task type\n",
    "\n",
    "# Set to 1 if crowdsourcing tasks in production should be created, else 0 for test\n",
    "CREATE_HITS_IN_PRODUCTION = 0\n",
    "\n",
    "# Set 1 if current taskSet (if existing) should be updated\n",
    "UPDATE_TASKSETS = 0\n",
    "\n",
    "# Set path to db credentials, used for storing crowdsourcing tasks and results \n",
    "PATH_MONGODB_CREDENTIALS = os.path.join(PROJECT_PATH, 'config/mongodb_credentials.json')\n",
    "\n",
    "# Set path to amazon credentials saved in a .json file\n",
    "PATH_AMAZON_CREDENTIALS = os.path.join(PROJECT_PATH, 'config/amazon_credentials.json')\n",
    "\n",
    "# Set Amazon mturk endpoint used for crowdsourcing experiments\n",
    "MTURK_ENDPOINT = 'https://mturk-requester.us-east-1.amazonaws.com'\n",
    "\n",
    "WORKER_BAN_LIST = os.path.join(PROJECT_PATH, 'config/banlist.json')\n",
    "\n",
    "# Path to save created tasks for a task \n",
    "PATH_PRE_TASKSETS = os.path.join(DATA_FOLDER, \"mturk\", \"tasksets\", \"{}_pre_tasksets.json\")\n",
    "PATH_PRE_TASKSETS_GOLD = os.path.join(DATA_FOLDER, \"mturk\", \"tasksets\", \"{}_pre_tasksets_gold.json\")\n",
    "PATH_PRE_TASKSETS_NON_GOLD = os.path.join(DATA_FOLDER, \"mturk\", \"tasksets\", \"{}_pre_tasksets_non_gold.json\")\n",
    "PATH_TASKSETS = os.path.join(DATA_FOLDER, \"mturk\", \"tasksets\", \"{}_tasksets.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9c506be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No worker banned.\n"
     ]
    }
   ],
   "source": [
    "# Ban workers which are spammers after pilot round \n",
    "\n",
    "to_ban = False\n",
    "if to_ban:\n",
    "    with open(WORKER_BAN_LIST,'r') as f:\n",
    "        banlist = json.load(f)\n",
    "    for w in banlist:\n",
    "        try:\n",
    "            print(w)\n",
    "            response = mt.client.create_worker_block(WorkerId=w, Reason='Malicious behaviour.')\n",
    "            assert(response['ResponseMetadata']['HTTPStatusCode'] == 200)\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            print(f\"Following exception thrown \", e)\n",
    "            continue\n",
    "    print(f\"{len(banlist)} workers banned in total.\")\n",
    "    \n",
    "else: \n",
    "    print(\"No worker banned.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5cb6a",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b46cf9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_QUALIFICATION_TESTS,'r') as f:\n",
    "    qualification_tests = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "145bd715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB connection\n",
    "\n",
    "with open(PATH_MONGODB_CREDENTIALS,'r') as f:\n",
    "    mongodb_credentials = json.load(f)\n",
    "\n",
    "# Connect to Mong\\oDB\n",
    "db_client = pymongo.MongoClient(mongodb_credentials[\"connection_string\"], tlsCAFile=certifi.where()) # connecting to database\n",
    "db = db_client['chartfc']\n",
    "\n",
    "hit_result_collection = db.hit_results if CREATE_HITS_IN_PRODUCTION else db.hit_results_sandbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11b8179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mturk client\n",
    "\n",
    "class MTurk():\n",
    "\n",
    "    def __init__(self):\n",
    "        with open(PATH_AMAZON_CREDENTIALS)  as f: # get the credentials from AMT \n",
    "            cfg = json.load(f)\n",
    "\n",
    "        self.access_key = cfg['access_key']\n",
    "        self.secret_key = cfg['secret_key']\n",
    "        \n",
    "        self.environments = {\n",
    "            \"production\": {\n",
    "                \"endpoint\": MTURK_ENDPOINT, # set mturk endpoint\n",
    "                \"preview\": \"https://www.mturk.com/mturk/preview\"\n",
    "            },\n",
    "            \"sandbox\": {\n",
    "                \"endpoint\": MTURK_ENDPOINT, # set mturk endpoint\n",
    "                \"preview\": \"https://workersandbox.mturk.com/mturk/preview\"\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def launch_client(self, production = False):\n",
    "        self.mturk_environment = self.environments[\"production\"] if production else self.environments[\"sandbox\"]\n",
    "        try:\n",
    "            session = boto3.Session(profile_name='mturk')\n",
    "        except botocore.exceptions.ProfileNotFound as e:\n",
    "            session = boto3.Session(\n",
    "                profile_name='mturk',\n",
    "                aws_access_key_id  = self.access_key,\n",
    "                aws_secret_access_key  = self.secret_key\n",
    "            )\n",
    "        self.client = session.client(\n",
    "            service_name= 'mturk',\n",
    "            region_name= 'us-east-1',\n",
    "            endpoint_url= self.mturk_environment['endpoint'],\n",
    "        )\n",
    "        print(self.client.get_account_balance()['AvailableBalance'])\n",
    "\n",
    "    def create_hit(self, html_layout, **TaskAttributes):\n",
    "        QUESTION_XML = \"\"\"...\"\"\"\n",
    "        question_xml = QUESTION_XML.format(html_layout)\n",
    "\n",
    "        response = self.client.create_hit(\n",
    "            **TaskAttributes,\n",
    "            Question=question_xml\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    def get_hit_status(self, HITId):\n",
    "        hit = self.client.get_hit(HITId=HITId)\n",
    "        hit_status = hit['HIT']['HITStatus']\n",
    "        return hit_status\n",
    "\n",
    "    def get_hit_answers(self, HITId, approve=False):\n",
    "\n",
    "        # Get list and number of Assignments that have been completed\n",
    "        hit_assignmentsList = self.client.list_assignments_for_hit(\n",
    "            HITId=HITId,\n",
    "            AssignmentStatuses=['Submitted','Approved']\n",
    "        )\n",
    "\n",
    "        assignments = hit_assignmentsList['Assignments']\n",
    "\n",
    "        # Get details and results of each Assignment and add to answers array\n",
    "        answers = []\n",
    "        for assignment in assignments:\n",
    "            worker_id = assignment['WorkerId']\n",
    "            assignment_id = assignment['AssignmentId']\n",
    "\n",
    "            answer_dict = xmltodict.parse(assignment['Answer'])['QuestionFormAnswers']['Answer']\n",
    "            values = {}\n",
    "            for entry in answer_dict:\n",
    "                try:\n",
    "                    values[entry['QuestionIdentifier']] = json.loads(entry['FreeText'])\n",
    "                except ValueError:\n",
    "                    values[entry['QuestionIdentifier']] = entry['FreeText']\n",
    "                except TypeError:\n",
    "                    values[entry['QuestionIdentifier']] = None\n",
    "\n",
    "            answer = {\n",
    "                'worker_id' : worker_id,\n",
    "                'assignment_id' : assignment_id,\n",
    "                'values' : values,\n",
    "                'HITId' : HITId\n",
    "            }\n",
    "            answers.append(answer)\n",
    "            \n",
    "            if approve:\n",
    "                # Approve or not assignments\n",
    "                if assignment['AssignmentStatus'] == 'Submitted':\n",
    "                    self.client.approve_assignment(\n",
    "                        AssignmentId = assignment_id,\n",
    "                        OverrideRejection = False\n",
    "                    )\n",
    "                    \n",
    "        return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4db5058a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000.00\n"
     ]
    }
   ],
   "source": [
    "# Connect to MTurk\n",
    "\n",
    "mt = MTurk()\n",
    "mt.launch_client(production = CREATE_HITS_IN_PRODUCTION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed9d5a",
   "metadata": {},
   "source": [
    "### Functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c0adcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSingleInstances = 1 # MAX TIMES ANY REFERENCE APPEARS AMONG THE TASK SETS\n",
    "\n",
    "def get_random_task(counter, n, non_gd_tasks):\n",
    "    \"\"\"Returns n random tasks of format (task, index) from list non_gd_tasks\"\"\"\n",
    "    \n",
    "    task_seq = []\n",
    "    p = [max(maxSingleInstances-c,0.0001) for c in counter] # indexes of samples which can be used for task generation \n",
    "    p = [pp/sum(p) for pp in p]\n",
    "    indexes = np.random.choice( # random choice of entries (given some constraints)\n",
    "        a = list(range(len(non_gd_tasks))),\n",
    "        size=n,\n",
    "        replace=False, \n",
    "        p=p # samples used before in other tasks => probability set to 0 so that they are not chosen twice\n",
    "    )        \n",
    "\n",
    "    for i in indexes:\n",
    "        task = non_gd_tasks[i]\n",
    "        task_seq.append((task,i))\n",
    "    \n",
    "    return task_seq\n",
    "\n",
    "\n",
    "def get_random_gd_task(n, gd_tasks):\n",
    "    \"\"\"Returns a sub-list of gd_tasks (ground truth) with n entries\"\"\"\n",
    "    return random.sample(gd_tasks, n)\n",
    "\n",
    "\n",
    "def generate_taskset(counter, non_gd_tasks, gd_tasks, n=(5,2)): # set the number of non-gold labelled tasks (e.g. 5) and gold labelled task (e.g. 2)\n",
    "    '''\n",
    "    counter = a counter which keeps track of how many times each reference was retrieved\n",
    "    n = (x,y) where x = number of non_gd references and y = number of gd references\n",
    "    '''\n",
    "    taskSet = []\n",
    "    \n",
    "    task = get_random_task(counter, n[0], non_gd_tasks) # returns list of (index, sample)\n",
    "    taskSet = [r for (r,i) in task] # pairs reference,index are generated here, so that we can update the counter later\n",
    "    taskSet.extend(get_random_gd_task(n[1], gd_tasks))\n",
    "    \n",
    "    random.shuffle(taskSet) # gold standard should occur anywhere\n",
    "    \n",
    "    return taskSet, [i for (p,i) in task] # indixes returned to update counter\n",
    "\n",
    "\n",
    "def generate_taskset_no_gold(counter, non_gd_tasks, n=5):\n",
    "    \"\"\"\n",
    "    Generates taskset of n (default = 5) tasks and updates counter,\n",
    "    so that a task appears only 'maxSingleInstances'-times in a taskSet\n",
    "    \"\"\"\n",
    "    task_index_set = get_random_task(counter, n, non_gd_tasks) # returns list of (index, sample)\n",
    "    taskSet = [r for (r,i) in task_index_set] # tasks extracted from the pairs of (task, index)\n",
    "    \n",
    "    return taskSet, [i for (p,i) in task_index_set] # indixes returned to update counter\n",
    "\n",
    "\n",
    "def generate_taskset_w_ids(non_gd_tasks, gd_tasks, non_gold_per_taskset, gold_per_taskset) -> list:\n",
    "    \"\"\"\n",
    "    Function to generate TaskSets given gold standards and other samples \n",
    "    \"\"\"\n",
    "    taskSets = []\n",
    "    counter = [0]*len(non_gd_tasks) # keeping track if sample has been used before \n",
    "\n",
    "    while (any([c < maxSingleInstances for c in counter])):\n",
    "        # generate task set with some random samples\n",
    "        taskSet, indexes = generate_taskset(counter, non_gd_tasks, gd_tasks, n=(non_gold_per_taskset, gold_per_taskset)) \n",
    "        taskSetIDd = {\n",
    "            '_id': str(uuid.uuid4()),\n",
    "            'taskSet' : taskSet\n",
    "        }\n",
    "        taskSets.append(taskSetIDd) # add created taskSet with ID to list of taskSets \n",
    "        for i in indexes:\n",
    "            # increase counter for samples added, so that they only appear 'maxSingleInstances'-times in a taskSet\n",
    "            counter[i] = counter[i] + 1 \n",
    "    \n",
    "    return taskSets, counter\n",
    "\n",
    "\n",
    "def generate_taskset_w_ids_no_gold(non_gd_tasks, n = 5) -> list:\n",
    "    \"\"\"\n",
    "    Function to generate TaskSets given samples but without gold labelled samples \n",
    "    n (int): number of non_gold entries per taskSet\n",
    "    \"\"\"\n",
    "    task_set_list = []\n",
    "    counter = [0]*len(non_gd_tasks) # keeping track if sample has been used before \n",
    "\n",
    "    while (any([c < maxSingleInstances for c in counter])): # iterate as long as any sample in non_gd_tasks as not been used\n",
    "        # generate task set with some random samples\n",
    "        task_set, indexes = generate_taskset_no_gold(counter, non_gd_tasks, n) \n",
    "        task_set_w_id = {\n",
    "            '_id': str(uuid.uuid4()),\n",
    "            'taskSet' : task_set\n",
    "        }\n",
    "        \n",
    "        task_set_list.append(task_set_w_id) # add created taskSet with ID to list of taskSets \n",
    "        for i in indexes:\n",
    "            # increase counter for samples added, so that they only appear 'maxSingleInstances'-times in a taskSet\n",
    "            counter[i] = counter[i] + 1 \n",
    "    \n",
    "    return task_set_list, counter\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee21c4",
   "metadata": {},
   "source": [
    "### 1. Generation of task sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cacf5681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of retrieved table: 2316\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>source</th>\n",
       "      <th>description</th>\n",
       "      <th>wikipedia_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6390685299c9ed38aa87098d</td>\n",
       "      <td>%25_of_A%26E_Attendees_Seen_within_Four_Hours_...</td>\n",
       "      <td>line_chart</td>\n",
       "      <td>//commons.wikimedia.org/wiki/File:%25_of_A%26E...</td>\n",
       "      <td>https://commons.wikimedia.org/wiki/user:Mattda...</td>\n",
       "      <td>A chart that shows the percentage of A&amp;E atten...</td>\n",
       "      <td>[https://en.wikipedia.org/wiki/Wrightington,_W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6390685299c9ed38aa87098e</td>\n",
       "      <td>-8_Freq._Polygon.JPG</td>\n",
       "      <td>line_chart</td>\n",
       "      <td>//commons.wikimedia.org/wiki/File:-8_Freq._Pol...</td>\n",
       "      <td></td>\n",
       "      <td>class assignment</td>\n",
       "      <td>[https://ca.wikibooks.org/wiki/Viquiprojecte:C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6390685299c9ed38aa87098f</td>\n",
       "      <td>-8_Ogive.JPG</td>\n",
       "      <td>line_chart</td>\n",
       "      <td>//commons.wikimedia.org/wiki/File:-8_Ogive.JPG</td>\n",
       "      <td></td>\n",
       "      <td>class assignment</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  6390685299c9ed38aa87098d   \n",
       "1  6390685299c9ed38aa87098e   \n",
       "2  6390685299c9ed38aa87098f   \n",
       "\n",
       "                                           file_name        type  \\\n",
       "0  %25_of_A%26E_Attendees_Seen_within_Four_Hours_...  line_chart   \n",
       "1                               -8_Freq._Polygon.JPG  line_chart   \n",
       "2                                       -8_Ogive.JPG  line_chart   \n",
       "\n",
       "                                                 url  \\\n",
       "0  //commons.wikimedia.org/wiki/File:%25_of_A%26E...   \n",
       "1  //commons.wikimedia.org/wiki/File:-8_Freq._Pol...   \n",
       "2     //commons.wikimedia.org/wiki/File:-8_Ogive.JPG   \n",
       "\n",
       "                                              source  \\\n",
       "0  https://commons.wikimedia.org/wiki/user:Mattda...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "\n",
       "                                         description  \\\n",
       "0  A chart that shows the percentage of A&E atten...   \n",
       "1                                   class assignment   \n",
       "2                                   class assignment   \n",
       "\n",
       "                                     wikipedia_pages  \n",
       "0  [https://en.wikipedia.org/wiki/Wrightington,_W...  \n",
       "1  [https://ca.wikibooks.org/wiki/Viquiprojecte:C...  \n",
       "2                                                 []  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from MongoDB database for taskset creation\n",
    "\n",
    "db_table = pd.DataFrame()\n",
    "\n",
    "if TASK_TYPE == \"chart_filtering\": \n",
    "    db_table = db.chart_filtering\n",
    "    cursor = db_table.find({})\n",
    "    db_table = pd.DataFrame(list(cursor))\n",
    "else:\n",
    "    print(f\"No table for task type {TASK_TYPE}\")\n",
    "    \n",
    "print(f\"Length of retrieved table: {len(db_table)}\")\n",
    "\n",
    "db_table.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cbca4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tasks_chart_filtering(df): \n",
    "    \"\"\"\n",
    "    Function to create task set for the task chart filtering \n",
    "    df: database table for which to create task sets\n",
    "    \"\"\"\n",
    "    \n",
    "    # template for chart filtering tasks\n",
    "    task_template = {\n",
    "        \"db_id\": \"\",\n",
    "        \"chart_img\": \"\"\n",
    "    }\n",
    "    \n",
    "    task_list = []\n",
    "    # Iterate over data in df and create task sets\n",
    "    for index, row in df.iterrows():\n",
    "        task = copy.deepcopy(task_template)\n",
    "        task[\"db_id\"] = str(row[\"_id\"])\n",
    "        task[\"chart_img\"] = row[\"file_name\"]\n",
    "        \n",
    "        task_list.append(task)\n",
    "        \n",
    "    print(f\"{len(task_list)} tasks in total created.\\n\")\n",
    "    return task_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "717b545e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2316 tasks in total created.\n",
      "\n",
      "Saving generated task list at path C:\\Users\\k20116188\\PycharmProjects\\chartfc_dataset_wikicommons\\data\\mturk\\tasksets\\chart_filtering_pre_tasksets.json.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and save tasks for chart filtering \n",
    "chart_filtering_tasks = create_tasks_chart_filtering(db_table)\n",
    "\n",
    "path = PATH_PRE_TASKSETS.format(TASK_TYPE)\n",
    "print(f\"Saving generated task list at path {path}.\\n\")\n",
    "\n",
    "with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(chart_filtering_tasks, file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb27b2",
   "metadata": {},
   "source": [
    "### Split tasks in gold and non-gold samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "73893a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_standard_quota = 50 # number of gold tasks we want\n",
    "\n",
    "with open(PATH_PRE_TASKSETS.format(TASK_TYPE), \"rb\") as file: \n",
    "    task_list = json.load(file)\n",
    "\n",
    "# shuffle list to select gold standards randomly \n",
    "random.shuffle(task_list)\n",
    "\n",
    "# save tasks for ground truth labelling \n",
    "with open(PATH_PRE_TASKSETS_GOLD.format(TASK_TYPE), 'w+', encoding='utf8') as file:\n",
    "    json.dump(task_list[:golden_standard_quota], file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# save other tasks seperately\n",
    "with open(PATH_PRE_TASKSETS_NON_GOLD.format(TASK_TYPE), 'w+', encoding='utf8') as file:\n",
    "    json.dump(task_list[golden_standard_quota:], file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a7e45",
   "metadata": {},
   "source": [
    "### TODO (!): Before next step: manually set gold labels in task_list_gold.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13884d59",
   "metadata": {},
   "source": [
    "#### Generating tasksets of 7 tasks each (two out of them are gold standards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d372e114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454 tasksets created out of 50 gold tasks and 2266 non-gold tasks.\n"
     ]
    }
   ],
   "source": [
    "# load task list of gold standards\n",
    "with open(PATH_PRE_TASKSETS_GOLD.format(TASK_TYPE), \"rb\") as file: \n",
    "    task_list_gold = json.load(file) \n",
    "\n",
    "# load non gold standard task list\n",
    "with open(PATH_PRE_TASKSETS_NON_GOLD.format(TASK_TYPE), \"rb\") as file: \n",
    "    task_list_non_gold = json.load(file) \n",
    "\n",
    "# set number of non-gold tasks to use accord. to how many labelled gold we have\n",
    "task_sets, counter = generate_taskset_w_ids(task_list_non_gold, task_list_gold, non_gold_per_taskset=5, gold_per_taskset=2)\n",
    "\n",
    "print(f\"{len(task_sets)} tasksets created out of {len(task_list_gold)} gold tasks and {len(task_list_non_gold)} non-gold tasks.\")\n",
    "\n",
    "# save tasksets \n",
    "with open(PATH_TASKSETS.format(TASK_TYPE), 'w+', encoding='utf8') as file:\n",
    "    json.dump(task_sets, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# NEEDED LATER FOR CLAIM WRITING TASKS \n",
    "\n",
    "# elif update_tasksets and task_type == \"claim_generation\":\n",
    "#     with open(os.path.join(data_folder, \"TaskSets/pre_tasksets_{}.json\".format(task_type)), \"rb\") as file: \n",
    "#         task_list_non_gold = json.load(file)\n",
    "    \n",
    "#     task_sets, counter = generate_taskset_w_ids_no_gold(task_list_non_gold, n = 5)\n",
    "#     print(f\"{len(task_sets)} tasksets created out of {len(task_list_non_gold)} non-gold tasks.\")\n",
    "    \n",
    "#     # save tasksets \n",
    "#     with open(os.path.join(data_folder, \"TaskSets/tasksets_{}.json\".format(task_type)), 'w+', encoding='utf8') as file:\n",
    "#         json.dump(task_sets, file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46440e96",
   "metadata": {},
   "source": [
    "### 2. Running Crowdsourcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to preprocess tables for displaying in annotation UI\n",
    "        \n",
    "def taskset_tostring(taskSet_list: list): \n",
    "    \"\"\"\n",
    "    Preprocess table representation before sending to UI\n",
    "    \"\"\"\n",
    "    new_list = []\n",
    "    for taskSet in taskSet_list:\n",
    "        html_table = taskSet['table']['html_table']\n",
    "\n",
    "        html_table_bs = BeautifulSoup(html_table)\n",
    "        html_table_bs = _remove_attrs(html_table_bs)\n",
    "        _remove_tags(html_table_bs)\n",
    "\n",
    "        for tag in html_table_bs.findAll([\"table\", \"th\", \"td\"]):\n",
    "            tag['style'] = \"border: 1px solid black;\"\n",
    "\n",
    "#         html_table = str(html_table_bs.body.table).replace(\"\\n\", \"\")\n",
    "#         html_table = str(html_table_bs.body.table).replace(\"\\n\", \"\").replace(\"'\", \"\\'\")\n",
    "\n",
    "        taskSet['table']['html_table'] = html_table\n",
    "    \n",
    "    result_str = str(taskSet_list).replace(\"\\'<table\", \"`<table\").replace(\"table>\\'\", \"table>`\").replace(\"\\xa0\", \" \").replace(\"\\'caption\\': None\", \"\\'caption\\': \\'no caption given\\'\")\n",
    "    result_str = result_str.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "    return result_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a55b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING GENERAL THINGS, UI, ETC. \n",
    "\n",
    "\"\"\" Create the tasks by populating the HTML templates using the config file \"\"\"\n",
    "\n",
    "with open(TASK_CONFIG.format(task_type),'r') as f: # load config file with worker qualifications, location, etc. \n",
    "    task_temp = json.load(f)\n",
    "    \n",
    "taskSets_all_lan = {}\n",
    "\n",
    "task_content = copy.deepcopy(task_temp)\n",
    "task_content['language'] = \"en\"\n",
    "\n",
    "TaskAttributes = task_content['task_attributes']\n",
    "\n",
    "with open('./config/' + task_content['instructions_project_text_file'],'r') as f:\n",
    "    task_content['instructions_project_text'] = f.read().replace('\\n',' ')\n",
    "with open('./config/' + task_content['instructions_rules_text_file'],'r') as f:\n",
    "    task_content['instructions_rules_text'] = f.read().replace('\\n',' ')\n",
    "\n",
    "html_layout = open(task_content['html_layout'], 'r').read()\n",
    "\n",
    "# enter instruction texts in the html template \n",
    "html_layout = html_layout.\\\n",
    "    replace('${instructions_project_text}$', task_content['instructions_project_text']).\\\n",
    "    replace('${instructions_rules_text}$', task_content['instructions_rules_text']).\\\n",
    "    replace('${time_thr}$', task_content['time_thr'])\n",
    "\n",
    "with open(data_folder + task_content['tasks'],'r') as f:\n",
    "    taskSets = json.load(f)\n",
    "\n",
    "# If you're only testing, just pick one hit and run it once, with no qualification barriers\n",
    "if not create_hits_in_production:\n",
    "    TaskAttributes.pop('QualificationRequirements')\n",
    "    TaskAttributes['MaxAssignments'] = 1 \n",
    "    random.seed(42)\n",
    "    #taskSets = random.sample(taskSets,1)\n",
    "\n",
    "print('Generated {} tasks with the following configs:'.format(len(taskSets)))\n",
    "pprint(TaskAttributes,indent=1) #verify the properties before running the HITs\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ff8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE HITS ARE CREATED (=> AMT CALLED) AND SAVED IN MONGODB WITH CORRESP. HIT_ID, later based on these HIT_ID the results are updated\n",
    "\n",
    "\"\"\" Create the batch of HITs \"\"\"\n",
    "\n",
    "results = []\n",
    "batch_id = str(uuid.uuid4())\n",
    "\n",
    "hit_type_id = ''\n",
    "target_assignments = TaskAttributes['MaxAssignments']\n",
    "\n",
    "for taskSet in taskSets[:10]: \n",
    "\n",
    "    TaskAttributes_hit = copy.deepcopy(TaskAttributes) # Adjust based on how many were already done in other batches\n",
    "    TaskAttributes_hit['MaxAssignments'] = target_assignments -\\\n",
    "        sum([hit['hit']['NumberOfAssignmentsCompleted'] for hit in hit_result_collection.find({\n",
    "            'taskSet_id':taskSet['_id'],\n",
    "            'type': task_content['type'],\n",
    "            'language': \"English\"\n",
    "        })])\n",
    "    if TaskAttributes_hit['MaxAssignments'] > 0:\n",
    "        random.seed(None)\n",
    "        language_questions = random.sample(qualification_tests,k=3) # Adjust with table questions\n",
    "        try:\n",
    "#             response = mt.create_hit(html_layout.replace('${references}$', mock_taskSet).\\\n",
    "#                                      replace('${lan_test_questions}$', json.dumps(language_questions)),\n",
    "#                                      **TaskAttributes_hit)\n",
    "            response = mt.create_hit(html_layout.replace('${references}$', taskset_tostring(taskSet['taskSet'])).\\\n",
    "                                     replace('${lan_test_questions}$', json.dumps(language_questions)),\n",
    "                                     **TaskAttributes_hit)\n",
    "            \n",
    "        except Exception as e: \n",
    "            print(f\"Exception occurred, continue with next entry in TaskSets: {e}\")\n",
    "            continue \n",
    "\n",
    "        hit_type_id = response['HIT']['HITTypeId']\n",
    "        result = {\n",
    "            '_id': response['HIT']['HITId'],\n",
    "            'batch_id': batch_id,\n",
    "            'type': task_content['type'],\n",
    "            'references': taskSet['taskSet'],\n",
    "            'language': \"English\",\n",
    "            'taskSet_id':taskSet['_id'],\n",
    "            'hit': response['HIT'],\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "        results.append(result)\n",
    "        try:\n",
    "            hit_result_collection.insert_one(result) # ADD hit with ID returned from AMT to my MongoDB database\n",
    "        except Exception:\n",
    "            print(result)\n",
    "            raise\n",
    "\n",
    "# For you to go to the HITs you just created and test them\n",
    "print('Launched tasks for table fact checking')\n",
    "if not create_hits_in_production:\n",
    "    print('You can view the HITs here:')\n",
    "    print(mt.mturk_environment['preview']+\"?groupId={}\".format(hit_type_id))\n",
    "else:\n",
    "    print('Launched! Good Luck!')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a7ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Update all non-disposed hits in the database with correct results '''\n",
    "\"\"\" Rejected assignments are ignored \"\"\"\n",
    "\n",
    "approve_payment = False # Set to true if automatically approve payment \n",
    "\n",
    "for hit in hit_result_collection.find({'hit.HITStatus': {'$not': {'$eq': 'Disposed'}}, \n",
    "                                       'timestamp': {'$gte': datetime(2020, 6, 30)}}):\n",
    "    \n",
    "    print('Updating',hit['_id'],end='\\r')\n",
    "    try:\n",
    "        hit_result_collection.update_one(\n",
    "            {'_id': hit['_id']},\n",
    "            {\n",
    "                \"$set\": {\n",
    "                    \"hit\": mt.client.get_hit(HITId = hit['_id'])['HIT'],\n",
    "                    'answers': mt.get_hit_answers(hit['_id'], approve=approve_payment)\n",
    "                }\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(e,end='\\n\\n')\n",
    "        continue\n",
    "print('Done'+(' '*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b708f8",
   "metadata": {},
   "source": [
    "#### Delete previously sent HITs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82db9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {'hit.HITStatus': {'$not': {'$eq': 'Disposed'}}}\n",
    "# query['type'] = \"claim_generation\"\n",
    "len(list(hit_result_collection.find(query)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990e8f7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" If you set 'force' to TRUE, it will abort mission and force an expiry in all HITs and then delete them.\n",
    "If you only want to remove the completed ones (make them Disposed so the update routine won't loop through tons of\n",
    "HITs), keep it as FALSE.\"\"\"\n",
    "\n",
    "force = True\n",
    "\n",
    "while True:\n",
    "    ''' Dispose all hits in the database '''\n",
    "    query = {'hit.HITStatus': {'$not': {'$eq': 'Disposed'}}, 'timestamp': {'$gte': datetime(2020, 6, 23)}}\n",
    "#     query[\"type\"] = \"claim_generation\"\n",
    "#     query = {'timestamp': {'$gte': datetime.datetime(2021, 6, 29)}}\n",
    "    \n",
    "    if not force:\n",
    "        query['hit.NumberOfAssignmentsPending'] = 0\n",
    "        query['hit.NumberOfAssignmentsAvailable'] = 0\n",
    "    elif force:\n",
    "        query['hit.NumberOfAssignmentsPending'] = 0\n",
    "#         query['hit.NumberOfAssignmentsCompleted'] = 0\n",
    "    \n",
    "    hit_result_collection_list = list(hit_result_collection.find(query))\n",
    "    print(f\"Length of retrieved HITs \", len(hit_result_collection_list))\n",
    "    \n",
    "    if (not force and len(hit_result_collection_list) == 0) or (force and mt.client.list_hits()['NumResults']==0):\n",
    "        print('Finished')\n",
    "        break\n",
    "        \n",
    "    for hit in hit_result_collection_list:\n",
    "        try:\n",
    "            mt.client.delete_hit(HITId = hit['_id'])\n",
    "            print('Removed',hit['_id'])\n",
    "        except Exception as e:\n",
    "            print(hit['_id'], e)\n",
    "            if force:\n",
    "                print(\"force\")\n",
    "                try:\n",
    "                    mt.client.update_expiration_for_hit(HITId = hit['_id'], ExpireAt=datetime(2018, 4, 10, 7, 22, 15))\n",
    "                    mt.client.get_hit(HITId = hit['_id'])\n",
    "                    mt.client.delete_hit(HITId = hit['_id'])\n",
    "                    print('Removed',hit['_id'])\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                    print(hit['_id'],e)\n",
    "            continue\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7880698",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_list = mt.client.list_hits(MaxResults=100)\n",
    "print(f\"Length of retrieved hits: {len(hit_list['HITs'])}\")\n",
    "force = True\n",
    "\n",
    "for hit in hit_list[\"HITs\"]:\n",
    "    hitid = hit[\"HITId\"]\n",
    "    try:\n",
    "        mt.client.update_expiration_for_hit(HITId = hitid, ExpireAt=datetime(2018, 1, 1))\n",
    "        mt.client.get_hit(HITId = hitid)\n",
    "        mt.client.delete_hit(HITId = hitid)\n",
    "        print('Removed',hitid)\n",
    "    except Exception as e:\n",
    "        print(f\"Following error occured while deleting HIT {hitid}, lets continue: {e}.\")\n",
    "        if force:\n",
    "            try:\n",
    "                mt.client.update_expiration_for_hit(hitid, ExpireAt=datetime(2017, 1, 1))\n",
    "                mt.client.delete_hit(HITId = hitid)\n",
    "                print('Removed', hitid)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "#                 print(hit['_id'],e)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41baf9fe",
   "metadata": {},
   "source": [
    "#### Delete specific Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "force = True\n",
    "\n",
    "while True:\n",
    "    ''' Dispose all hits in the database '''\n",
    "    \n",
    "#     if not force:\n",
    "#         query['hit.NumberOfAssignmentsPending'] = 0\n",
    "#         query['hit.NumberOfAssignmentsAvailable'] = 0\n",
    "#     elif force:\n",
    "#         query['hit.NumberOfAssignmentsPending'] = 0\n",
    "    \n",
    "    hit_result_collection_list = [...] # TODO enter here HIT_IDs which should be deleted\n",
    "    \n",
    "#     if (not force and len(hit_result_collection_list) == 0) or (force and mt.client.list_hits()['NumResults']==0):\n",
    "#         print('Finished')\n",
    "#         break\n",
    "        \n",
    "    for hit_id in hit_result_collection_list:\n",
    "        try:\n",
    "            mt.client.update_expiration_for_hit(HITId = hitid, ExpireAt=datetime(2018, 1, 1))\n",
    "            x = mt.client.delete_hit(HITId = hit_id)\n",
    "            print('Removed',hit_id)\n",
    "        except Exception as e:\n",
    "            print(hit_id)\n",
    "            if force:\n",
    "                print(\"force\")\n",
    "                try:\n",
    "                    mt.client.update_expiration_for_hit(HITId = hit_id, ExpireAt=datetime(2017, 1, 1))\n",
    "                    mt.client.delete_hit(HITId = hit_id)\n",
    "                    print('Removed',hit_id)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                    print(hit_id,e)\n",
    "            continue\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e4b64d",
   "metadata": {},
   "source": [
    "#### Update HITs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cecedc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitid = '...' # TODO enter HIT_ID you want to update\n",
    "\n",
    "mt.client.update_expiration_for_hit(HITId = hitid, ExpireAt=datetime(2015, 1, 1))\n",
    "\n",
    "# mt.client.update_hit_review_status(HITId = hitid, Revert=True)\n",
    "# mt.client.update_hit_type_of_hit(HITId = hitid, HITTypeId='1623495307575')\n",
    "\n",
    "# mt.get_hit_answers(hitid, approve=True)\n",
    "pprint(mt.client.get_hit(HITId = hitid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89654148",
   "metadata": {},
   "source": [
    "### Preparing html tables for UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d35290",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_folder + \"TaskSets/task_sets_en_table.json\",'r') as f:\n",
    "    task_sets = json.load(f)\n",
    "    \n",
    "subset = []\n",
    "for task_set in task_sets: \n",
    "    for task in task_set['taskSet']: \n",
    "        if (task['table']['header_horizontal']!=[] and any(task['table']['header_horizontal'])) or \\\n",
    "        (task['table']['header_vertical']!=[] and any(task['table']['header_vertical'])): \n",
    "            subset.append(task)\n",
    "            \n",
    "len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef78ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "from lxml.etree import tostring\n",
    "\n",
    "\n",
    "def del_col_row(table, row_i = None, col_i = None):\n",
    "    \"\"\"remove columns and rows from table\"\"\"\n",
    "\n",
    "    if type(table)==str:\n",
    "        table = html.fragment_fromstring(table)\n",
    "        \n",
    "    # remove column i\n",
    "    if col_i != None:\n",
    "        for row in table.getchildren()[0].iterchildren():\n",
    "            row.remove(row.getchildren()[col_i])\n",
    "        \n",
    "    # remove row i\n",
    "    if row_i != None:\n",
    "        for index, row in zip(range(len(table.getchildren()[0].getchildren())), table.getchildren()[0].iterchildren()):\n",
    "            if index == row_i:\n",
    "                row.getparent().remove(row)\n",
    "    \n",
    "    return table \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE 1\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_income\"\n",
    "resp = requests.get(url)\n",
    "page = BeautifulSoup(resp.text, 'html.parser')\n",
    "tables = page.findAll(\"table\")\n",
    "\n",
    "table = tables[3]\n",
    "\n",
    "# TODO replace in function del_col_row() e.g. table.getchildren() by table.getchildren()[0].getchildren()\n",
    "# drop columns and rows unnecessary \n",
    "table = html.tostring(del_col_row(str(table), col_i = 3)).decode('utf-8')\n",
    "table = html.tostring(del_col_row(str(table), col_i = 3)).decode('utf-8')\n",
    "table = html.tostring(del_col_row(str(table), col_i = 4)).decode('utf-8')\n",
    "table = html.tostring(del_col_row(str(table), col_i = 4)).decode('utf-8')\n",
    "table = html.tostring(del_col_row(str(table), col_i = 0)).decode('utf-8')\n",
    "\n",
    "for i in range(8):\n",
    "    table = html.tostring(del_col_row(str(table), row_i = 2)).decode('utf-8')\n",
    "for i in range(6):\n",
    "    table = html.tostring(del_col_row(str(table), row_i = 9)).decode('utf-8')\n",
    "for i in range(35):\n",
    "    table = html.tostring(del_col_row(str(table), row_i = 13)).decode('utf-8')\n",
    "    \n",
    "# BeautifulSoup(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e3231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE 2\n",
    "\n",
    "url = \"https://www.cdc.gov/flu/about/burden/index.html\"\n",
    "resp = requests.get(url)\n",
    "page = BeautifulSoup(resp.text, 'html.parser')\n",
    "tables = page.findAll(\"table\")\n",
    "\n",
    "table = tables[0]\n",
    "\n",
    "# drop columns and rows unnecessary \n",
    "# table = html.tostring(del_col_row(str(table), col_i = None)).decode('utf-8')\n",
    "\n",
    "for i in range(8):\n",
    "    table = html.tostring(del_col_row(str(table), row_i = 0)).decode('utf-8')\n",
    "    \n",
    "table = html.tostring(del_col_row(str(table), col_i = 2)).decode('utf-8')\n",
    "table = html.tostring(del_col_row(str(table), col_i = 3)).decode('utf-8')\n",
    "table = html.tostring(del_col_row(str(table), col_i = 4)).decode('utf-8')\n",
    "table = html.tostring(del_col_row(str(table), col_i = 5)).decode('utf-8')\n",
    "\n",
    "# Delete second header row.. \n",
    "table = html.fragment_fromstring(table)\n",
    "table.getchildren()[1].remove(table.getchildren()[1].getchildren()[1])\n",
    "# table.getchildren()[1].remove(table.getchildren()[1].getchildren()[0])\n",
    "\n",
    "table = html.tostring(table).decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a6543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE 3\n",
    "\n",
    "url = \"https://www.macrotrends.net/countries/AUS/australia/crime-rate-statistics\"\n",
    "resp = requests.get(url)\n",
    "page = BeautifulSoup(resp.text, 'html.parser')\n",
    "tables = page.findAll(\"table\")\n",
    "table = tables[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE 4\n",
    "\n",
    "url = \"https://www.cebm.net/covid-19/global-covid-19-case-fatality-rates\"\n",
    "resp = requests.get(url)\n",
    "page = BeautifulSoup(resp.text, 'html.parser')\n",
    "tables = page.findAll(\"table\")\n",
    "table = tables[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE 5\n",
    "\n",
    "url = \"https://www.nimh.nih.gov/health/statistics/suicide.shtml\"\n",
    "resp = requests.get(url)\n",
    "page = BeautifulSoup(resp.text, 'html.parser')\n",
    "tables = page.findAll(\"table\")\n",
    "table = tables[0]\n",
    "\n",
    "for i in range(5):\n",
    "    table = html.tostring(del_col_row(str(table), row_i = 5)).decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3addd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table Test\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Demographics_of_the_United_States\"\n",
    "resp = requests.get(url)\n",
    "page = BeautifulSoup(resp.text, 'html.parser')\n",
    "tables = page.findAll(\"table\")\n",
    "table = tables[23]\n",
    "\n",
    "for i in range(5):\n",
    "    table = html.tostring(del_col_row(str(table), row_i = 10)).decode('utf-8')\n",
    "\n",
    "table = html.tostring(del_col_row(str(table), row_i = 0)).decode('utf-8')\n",
    "table = html.tostring(del_col_row(str(table), col_i = 0)).decode('utf-8')\n",
    "\n",
    "table = html.fragment_fromstring(table)\n",
    "table.getchildren()[0].getchildren()[0].remove(table.getchildren()[0].getchildren()[0].getchildren()[4])\n",
    "\n",
    "for row in table.getchildren()[0].iterchildren():\n",
    "    if row.getchildren():\n",
    "#         print(f\"This: {html.tostring(row.getchildren()[0])}\")\n",
    "        row.remove(row.getchildren()[2])\n",
    "        \n",
    "# table = html.tostring(del_col_row(str(table), col_i = 0)).decode('utf-8')\n",
    "\n",
    "table = html.tostring(table).decode('utf-8')\n",
    "BeautifulSoup(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = subset[100]\n",
    "\n",
    "# if not task['table']['caption']:\n",
    "#     task['table']['caption'] = ''\n",
    "# html_table = task['table']['html_table']\n",
    "\n",
    "html_table_bs = BeautifulSoup(table)\n",
    "# html_table_bs = table\n",
    "html_table_bs = _remove_attrs(html_table_bs)\n",
    "_remove_img(html_table_bs)\n",
    "_remove_caption(html_table_bs)\n",
    "\n",
    "for tag in html_table_bs.findAll([\"table\", \"th\", \"td\"]):\n",
    "    tag['style'] = \"border: 1px solid black;\"\n",
    "\n",
    "html_table = str(html_table_bs).replace(\"\\n\", \"\").replace(\"'\", \"\\'\")\n",
    "# html_table_bs = BeautifulSoup(html_table)\n",
    "\n",
    "print(html_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd91d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load taskset for table task for MOCK HTML page\n",
    "\n",
    "\n",
    "html_table = task_sets[i][\"taskSet\"][entry]['table']['html_table']\n",
    "html_table = html_table.replace(\"\\n\", \"\")\n",
    "html_table = html_table.replace(\"'\", \"\\'\")\n",
    "# html_table = html_table.replace(\"%\", \"\\%\")\n",
    "# html_table = html_table.replace(\"\", \"\\'\")\n",
    "\n",
    "html_table_bs = BeautifulSoup(html_table)\n",
    "html_table_bs = _remove_attrs(html_table_bs)\n",
    "html_table_bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d85d90",
   "metadata": {},
   "source": [
    "### 3. Updating HITs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b09649",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_table_list = []\n",
    "tags = []\n",
    "\n",
    "for table_list in test_data[\"tables_wikipedia_references\"]:\n",
    "    if table_list and type(table_list)!=float:\n",
    "        for table in table_list: \n",
    "            if table and \"html_table\" in table and type(table[\"html_table\"])==str:\n",
    "                soup = BeautifulSoup(table[\"html_table\"])\n",
    "                tags.extend([tag.name for tag in soup.find_all()])\n",
    "                html_table_list.append(table[\"html_table\"])\n",
    "            \n",
    "len(html_table_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f649e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = list(set(tags))\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9c94c",
   "metadata": {},
   "source": [
    "#### Send some taskSets again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8537df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'./data/production/TaskSets/final_table_annotation/tasksets_table_annotation_1.json', \"r\") as file: \n",
    "    taskset_1 = json.load(file)\n",
    "    \n",
    "with open(r'./data/production/TaskSets/final_table_annotation/tasksets_table_annotation_3.json', \"r\") as file: \n",
    "    taskset_3 = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04ba79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_taskSets = []\n",
    "\n",
    "for entry in taskset_1+taskset_3: \n",
    "    for task in entry[\"taskSet\"]:\n",
    "        if task[\"claim_db_id\"] == \"6072bd2a000ca92c09d11fb5\":\n",
    "            task[\"table\"][\"header_horizontal\"] = []\n",
    "            task[\"g_id\"] = 3\n",
    "            relevant_taskSets.append(entry)\n",
    "            \n",
    "        elif task[\"claim_db_id\"] == \"6072bd2d000ca92c09d145b8\":\n",
    "            task[\"g_id\"] = 3\n",
    "            relevant_taskSets.append(entry)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee194de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'./data/production/TaskSets/final_table_annotation/tasksets_table_annotation_faultyHITs_updated.json', \"w\", encoding=\"utf-8\") as file: \n",
    "    json.dump(relevant_taskSets, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd135f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
