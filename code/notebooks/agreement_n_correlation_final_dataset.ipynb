{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load python packages\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import jsonlines\n",
    "import krippendorff\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from scipy.stats import chi2_contingency, kendalltau, pearsonr, spearmanr\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.info(\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "This notebook calculates the worker agreement scores and correlations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <strong>Load annotations for a task</strong> \n",
    "\n",
    "2. <strong>Run analysis for agreement among workers</strong> \n",
    "\n",
    "3. <strong>Run analysis for correlations among collected data</strong> \n",
    "\n",
    "\n",
    "Set the following variables first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x7/b6tgwcrn1rz_zffbw59v8crr0000gp/T/ipykernel_5093/3655114868.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load annotations into pd.Dataframe 'df'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Set path to PubHealthTab dataset (dataset.jsonl)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \"\"\"  # noqa: E501\n\u001b[1;32m    186\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "# Set one of the following options: \"table_annotation\", \"adjusted_claim_annotation\"\n",
    "task_type = \"chart_filtering\"\n",
    "\n",
    "# Load annotations into pd.Dataframe 'df' \n",
    "df = pd.read_pickle(\"\")\n",
    "\n",
    "# Set path to PubHealthTab dataset (dataset.jsonl) \n",
    "path_pubhealthtab = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the task to be postprocessed: \"claim_explanation_verification\", \"chart_filtering\"\n",
    "task = \"claim_explanation_verification\"\n",
    "\n",
    "# Load file with annotations into pandas.DataFrame OR if annotations saved in DB, create client and load data, example below \n",
    "df = pd.DataFrame()\n",
    "\n",
    "# PROJECT_PATH = r\"/Users/user/Library/CloudStorage/OneDrive-King'sCollegeLondon/PycharmProjects/chartfc_dataset_wikicommons\"\n",
    "# PATH_MONGODB_CREDENTIALS = os.path.join(PROJECT_PATH, 'config/mongodb_credentials.json')\n",
    "# PATH_BANNED_WORKERS = os.path.join(PROJECT_PATH, 'config/banlist.json')\n",
    "\n",
    "# PATH_BANNED_WORKERS = os.path.join(PROJECT_PATH, 'config/banlist_claim_explanation_generation.json')\n",
    "PATH_BANNED_WORKERS = os.path.join(PROJECT_PATH, 'config/banlist_claim_explanation_verification.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in 'hit_result_collection' is 1869\n"
     ]
    }
   ],
   "source": [
    "# Load annotation data from MongoDB \n",
    "\n",
    "with open(PATH_MONGODB_CREDENTIALS,'r') as f:\n",
    "    mongodb_credentials = json.load(f)\n",
    "\n",
    "db_client = pymongo.MongoClient(mongodb_credentials[\"connection_string\"])\n",
    "db = db_client['chartfc']\n",
    "\n",
    "if task == \"chart_filtering\":\n",
    "    hit_result_collection = db.hit_results\n",
    "elif task == \"claim_explanation_verification\":\n",
    "    hit_result_collection = db.hit_results_claim_explanation_verification\n",
    "\n",
    "cursor = hit_result_collection.find({})\n",
    "df = pd.DataFrame(list(cursor))\n",
    "\n",
    "print(f\"Number of samples in 'hit_result_collection' is {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>type</th>\n",
       "      <th>references</th>\n",
       "      <th>taskSet_id</th>\n",
       "      <th>hit</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3THR0FZ96WRCINKMS5SPWCYMLAXLON</td>\n",
       "      <td>4e982d1c-42e8-48d5-a3cd-e893899dbd2c</td>\n",
       "      <td>claim_explanation_verification</td>\n",
       "      <td>[{'db_id': '3J94SKDELPN7C1QCA9Y24TQQVRJD5T', '...</td>\n",
       "      <td>085847c3-da58-46fa-8fd0-3ce78f6dcfda</td>\n",
       "      <td>{'HITId': '3THR0FZ96WRCINKMS5SPWCYMLAXLON', 'H...</td>\n",
       "      <td>2023-04-27 12:47:33.500</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3YOAVL4CB7FSJJR8OM4PQT3FJSXZ42</td>\n",
       "      <td>ec997789-b359-467c-8e94-42074979d084</td>\n",
       "      <td>claim_explanation_verification</td>\n",
       "      <td>[{'db_id': '34OWYT6U43F7LZ8A5DCU76B59XHI98', '...</td>\n",
       "      <td>e971fd49-3e31-4b2b-99d7-96f4f2761063</td>\n",
       "      <td>{'HITId': '3YOAVL4CB7FSJJR8OM4PQT3FJSXZ42', 'H...</td>\n",
       "      <td>2023-04-28 10:35:33.226</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3QGTX7BCIW0AF2V82RNZVL9TJOSZ57</td>\n",
       "      <td>ec997789-b359-467c-8e94-42074979d084</td>\n",
       "      <td>claim_explanation_verification</td>\n",
       "      <td>[{'db_id': '30UZJB2PPOA9U9OFHKSQRK4E4D653X', '...</td>\n",
       "      <td>58238466-7d19-45c2-8f54-85a1e0beb9dc</td>\n",
       "      <td>{'HITId': '3QGTX7BCIW0AF2V82RNZVL9TJOSZ57', 'H...</td>\n",
       "      <td>2023-04-28 10:35:34.507</td>\n",
       "      <td>[{'worker_id': 'A22IXLFA45IJJ2', 'assignment_i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              _id                              batch_id  \\\n",
       "0  3THR0FZ96WRCINKMS5SPWCYMLAXLON  4e982d1c-42e8-48d5-a3cd-e893899dbd2c   \n",
       "1  3YOAVL4CB7FSJJR8OM4PQT3FJSXZ42  ec997789-b359-467c-8e94-42074979d084   \n",
       "2  3QGTX7BCIW0AF2V82RNZVL9TJOSZ57  ec997789-b359-467c-8e94-42074979d084   \n",
       "\n",
       "                             type  \\\n",
       "0  claim_explanation_verification   \n",
       "1  claim_explanation_verification   \n",
       "2  claim_explanation_verification   \n",
       "\n",
       "                                          references  \\\n",
       "0  [{'db_id': '3J94SKDELPN7C1QCA9Y24TQQVRJD5T', '...   \n",
       "1  [{'db_id': '34OWYT6U43F7LZ8A5DCU76B59XHI98', '...   \n",
       "2  [{'db_id': '30UZJB2PPOA9U9OFHKSQRK4E4D653X', '...   \n",
       "\n",
       "                             taskSet_id  \\\n",
       "0  085847c3-da58-46fa-8fd0-3ce78f6dcfda   \n",
       "1  e971fd49-3e31-4b2b-99d7-96f4f2761063   \n",
       "2  58238466-7d19-45c2-8f54-85a1e0beb9dc   \n",
       "\n",
       "                                                 hit               timestamp  \\\n",
       "0  {'HITId': '3THR0FZ96WRCINKMS5SPWCYMLAXLON', 'H... 2023-04-27 12:47:33.500   \n",
       "1  {'HITId': '3YOAVL4CB7FSJJR8OM4PQT3FJSXZ42', 'H... 2023-04-28 10:35:33.226   \n",
       "2  {'HITId': '3QGTX7BCIW0AF2V82RNZVL9TJOSZ57', 'H... 2023-04-28 10:35:34.507   \n",
       "\n",
       "                                             answers  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2  [{'worker_id': 'A22IXLFA45IJJ2', 'assignment_i...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of banned workers is 12.\n"
     ]
    }
   ],
   "source": [
    "# load list of banned workers to exclude them\n",
    "with open(PATH_BANNED_WORKERS,'r') as f:\n",
    "    banlist = json.load(f)\n",
    "\n",
    "# load list of rejected assignments to exclude them (if any exist)\n",
    "# rejected_assignments = pd.read_excel(\"filled_answer_df.xlsx\")\n",
    "# rejected_assignments = list(rejected_assignments[rejected_assignments[\"reject\"]==1].assignment_id)\n",
    "\n",
    "print(f\"Number of banned workers is {len(banlist)}.\")\n",
    "# print(f\"Number of rejected assignments is {len(rejected_assignments)}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = list(df.columns)\n",
    "cols.remove(\"type\")\n",
    "cols.remove(\"timestamp\")\n",
    "# cols.remove(\"answers\")\n",
    "cols.extend(['worker_id', 'outputs', 'times', 'events', 'feedback'])\n",
    "\n",
    "annotations_df = pd.DataFrame(columns = cols)\n",
    "counter_skip = 0\n",
    "index = 0\n",
    "for i, row in df.iterrows(): \n",
    "    if task != \"claim_generation\" and (type(row[\"answers\"])!=list or len(row[\"answers\"]) < 3): # we only consider df entries with completed assignments 3/3\n",
    "        counter_skip += 1\n",
    "        continue \n",
    "\n",
    "#     if type(row[\"answers\"])!=list or len(row[\"answers\"]) == 0: # we only consider df entries with completed assignments 3/3\n",
    "#         counter_skip += 1\n",
    "#         continue \n",
    "    \n",
    "    for worker_answer in row[\"answers\"]:\n",
    "        #if worker_answer[\"worker_id\"] in banlist or worker_answer[\"assignment_id\"] in rejected_assignments:\n",
    "        #    print(\"Skipped because worker is banned or assignment has been rejected.\")\n",
    "        #    counter_skip += 1\n",
    "        #    continue\n",
    "        \n",
    "        row[\"worker_id\"] = worker_answer[\"worker_id\"]\n",
    "        annotations_df.at[index, \"_id\"] = row[\"_id\"]\n",
    "        annotations_df.at[index, \"batch_id\"] = row[\"batch_id\"]\n",
    "        annotations_df.at[index, \"references\"] = row[\"references\"]\n",
    "        annotations_df.at[index, \"taskSet_id\"] = row[\"taskSet_id\"]\n",
    "        annotations_df.at[index, \"hit\"] = row[\"hit\"]\n",
    "        \n",
    "        annotations_df.at[index, \"worker_id\"] = worker_answer[\"worker_id\"]\n",
    "        annotations_df.at[index, \"assignment_id\"] = worker_answer[\"assignment_id\"]\n",
    "        annotations_df.at[index, \"outputs\"] = worker_answer[\"values\"][\"outputs\"]\n",
    "        annotations_df.at[index, \"times\"] = worker_answer[\"values\"][\"times\"]\n",
    "        annotations_df.at[index, \"events\"] = worker_answer[\"values\"][\"events\"]\n",
    "        annotations_df.at[index, \"feedback\"] = worker_answer[\"values\"][\"feedback\"]\n",
    "        index += 1\n",
    "\n",
    "print(len(annotations_df))\n",
    "counter_skip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>references</th>\n",
       "      <th>taskSet_id</th>\n",
       "      <th>hit</th>\n",
       "      <th>answers</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>outputs</th>\n",
       "      <th>times</th>\n",
       "      <th>events</th>\n",
       "      <th>feedback</th>\n",
       "      <th>assignment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3HEA4ZVWWKKC8OBJV3GJ6H2YIVF55D</td>\n",
       "      <td>83418db6-f464-48d4-b513-755f6b6df424</td>\n",
       "      <td>[{'db_id': '37Y5RYYI1W3MG9T4D5CMRB3APX9XSV', '...</td>\n",
       "      <td>f80e02a2-38db-4285-937d-647672503dd1</td>\n",
       "      <td>{'HITId': '3HEA4ZVWWKKC8OBJV3GJ6H2YIVF55D', 'H...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASLGN9PS93UYR</td>\n",
       "      <td>[{'label_claim': 0, 'label_explanation': 0, 'h...</td>\n",
       "      <td>[12691, 23636, 114491, 5881, 5550, 5160, 7269]</td>\n",
       "      <td>[{'timestamp': '2023-05-05T15:01:51.375Z', 'ty...</td>\n",
       "      <td>GOOD</td>\n",
       "      <td>32VNZTT0AF2M3C3B47FESAX6KKFR4G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3HEA4ZVWWKKC8OBJV3GJ6H2YIVF55D</td>\n",
       "      <td>83418db6-f464-48d4-b513-755f6b6df424</td>\n",
       "      <td>[{'db_id': '37Y5RYYI1W3MG9T4D5CMRB3APX9XSV', '...</td>\n",
       "      <td>f80e02a2-38db-4285-937d-647672503dd1</td>\n",
       "      <td>{'HITId': '3HEA4ZVWWKKC8OBJV3GJ6H2YIVF55D', 'H...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A32YOD79DL837P</td>\n",
       "      <td>[{'label_claim': 0, 'label_explanation': 0, 'h...</td>\n",
       "      <td>[6947, 3974, 4021, 4389, 3470, 5455, 16126]</td>\n",
       "      <td>[{'timestamp': '2023-05-06T02:36:30.258Z', 'ty...</td>\n",
       "      <td>good</td>\n",
       "      <td>3EG49X3512AKFVT1WUELEQ38HV8X6B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3HEA4ZVWWKKC8OBJV3GJ6H2YIVF55D</td>\n",
       "      <td>83418db6-f464-48d4-b513-755f6b6df424</td>\n",
       "      <td>[{'db_id': '37Y5RYYI1W3MG9T4D5CMRB3APX9XSV', '...</td>\n",
       "      <td>f80e02a2-38db-4285-937d-647672503dd1</td>\n",
       "      <td>{'HITId': '3HEA4ZVWWKKC8OBJV3GJ6H2YIVF55D', 'H...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A2Z0OH990725SA</td>\n",
       "      <td>[{'label_claim': 0, 'label_explanation': 0, 'h...</td>\n",
       "      <td>[11045, 5807, 4454, 4311, 4621, 3710, 7438]</td>\n",
       "      <td>[{'timestamp': '2023-05-06T04:35:29.926Z', 'ty...</td>\n",
       "      <td>GOOD</td>\n",
       "      <td>304SM51WAB2IPQOXYBP6QY88HI5SBA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              _id                              batch_id  \\\n",
       "0  3HEA4ZVWWKKC8OBJV3GJ6H2YIVF55D  83418db6-f464-48d4-b513-755f6b6df424   \n",
       "1  3HEA4ZVWWKKC8OBJV3GJ6H2YIVF55D  83418db6-f464-48d4-b513-755f6b6df424   \n",
       "2  3HEA4ZVWWKKC8OBJV3GJ6H2YIVF55D  83418db6-f464-48d4-b513-755f6b6df424   \n",
       "\n",
       "                                          references  \\\n",
       "0  [{'db_id': '37Y5RYYI1W3MG9T4D5CMRB3APX9XSV', '...   \n",
       "1  [{'db_id': '37Y5RYYI1W3MG9T4D5CMRB3APX9XSV', '...   \n",
       "2  [{'db_id': '37Y5RYYI1W3MG9T4D5CMRB3APX9XSV', '...   \n",
       "\n",
       "                             taskSet_id  \\\n",
       "0  f80e02a2-38db-4285-937d-647672503dd1   \n",
       "1  f80e02a2-38db-4285-937d-647672503dd1   \n",
       "2  f80e02a2-38db-4285-937d-647672503dd1   \n",
       "\n",
       "                                                 hit answers       worker_id  \\\n",
       "0  {'HITId': '3HEA4ZVWWKKC8OBJV3GJ6H2YIVF55D', 'H...     NaN   ASLGN9PS93UYR   \n",
       "1  {'HITId': '3HEA4ZVWWKKC8OBJV3GJ6H2YIVF55D', 'H...     NaN  A32YOD79DL837P   \n",
       "2  {'HITId': '3HEA4ZVWWKKC8OBJV3GJ6H2YIVF55D', 'H...     NaN  A2Z0OH990725SA   \n",
       "\n",
       "                                             outputs  \\\n",
       "0  [{'label_claim': 0, 'label_explanation': 0, 'h...   \n",
       "1  [{'label_claim': 0, 'label_explanation': 0, 'h...   \n",
       "2  [{'label_claim': 0, 'label_explanation': 0, 'h...   \n",
       "\n",
       "                                            times  \\\n",
       "0  [12691, 23636, 114491, 5881, 5550, 5160, 7269]   \n",
       "1     [6947, 3974, 4021, 4389, 3470, 5455, 16126]   \n",
       "2     [11045, 5807, 4454, 4311, 4621, 3710, 7438]   \n",
       "\n",
       "                                              events feedback  \\\n",
       "0  [{'timestamp': '2023-05-05T15:01:51.375Z', 'ty...     GOOD   \n",
       "1  [{'timestamp': '2023-05-06T02:36:30.258Z', 'ty...     good   \n",
       "2  [{'timestamp': '2023-05-06T04:35:29.926Z', 'ty...     GOOD   \n",
       "\n",
       "                    assignment_id  \n",
       "0  32VNZTT0AF2M3C3B47FESAX6KKFR4G  \n",
       "1  3EG49X3512AKFVT1WUELEQ38HV8X6B  \n",
       "2  304SM51WAB2IPQOXYBP6QY88HI5SBA  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 'g_id_claim' for index 0\n",
      "Error 'g_id_claim' for index 0\n",
      "Error 'g_id_claim' for index 0\n",
      "Error 'g_id_claim' for index 0\n",
      "Error 'g_id_claim' for index 0\n",
      "Error 'g_id_claim' for index 0\n",
      "Error 'g_id_claim' for index 0\n",
      "Error 'g_id_claim' for index 1\n",
      "Error 'g_id_claim' for index 1\n",
      "Error 'g_id_claim' for index 1\n",
      "Error 'g_id_claim' for index 1\n",
      "Error 'g_id_claim' for index 1\n",
      "Error 'g_id_claim' for index 1\n",
      "Error 'g_id_claim' for index 1\n",
      "Error 'g_id_claim' for index 2\n",
      "Error 'g_id_claim' for index 2\n",
      "Error 'g_id_claim' for index 2\n",
      "Error 'g_id_claim' for index 2\n",
      "Error 'g_id_claim' for index 2\n",
      "Error 'g_id_claim' for index 2\n",
      "Error 'g_id_claim' for index 2\n",
      "Error 'g_id_claim' for index 3\n",
      "Error 'g_id_claim' for index 3\n",
      "Error 'g_id_claim' for index 3\n",
      "Error 'g_id_claim' for index 3\n",
      "Error 'g_id_claim' for index 3\n",
      "Error 'g_id_claim' for index 3\n",
      "Error 'g_id_claim' for index 3\n",
      "Error 'g_id_claim' for index 4\n",
      "Error 'g_id_claim' for index 4\n",
      "Error 'g_id_claim' for index 4\n",
      "Error 'g_id_claim' for index 4\n",
      "Error 'g_id_claim' for index 4\n",
      "Error 'g_id_claim' for index 4\n",
      "Error 'g_id_claim' for index 4\n",
      "Error 'g_id_claim' for index 5\n",
      "Error 'g_id_claim' for index 5\n",
      "Error 'g_id_claim' for index 5\n",
      "Error 'g_id_claim' for index 5\n",
      "Error 'g_id_claim' for index 5\n",
      "Error 'g_id_claim' for index 5\n",
      "Error 'g_id_claim' for index 5\n",
      "Error 'g_id_claim' for index 6\n",
      "Error 'g_id_claim' for index 6\n",
      "Error 'g_id_claim' for index 6\n",
      "Error 'g_id_claim' for index 6\n",
      "Error 'g_id_claim' for index 6\n",
      "Error 'g_id_claim' for index 6\n",
      "Error 'g_id_claim' for index 6\n",
      "Error 'g_id_claim' for index 7\n",
      "Error 'g_id_claim' for index 7\n",
      "Error 'g_id_claim' for index 7\n",
      "Error 'g_id_claim' for index 7\n",
      "Error 'g_id_claim' for index 7\n",
      "Error 'g_id_claim' for index 7\n",
      "Error 'g_id_claim' for index 7\n",
      "Error 'g_id_claim' for index 8\n",
      "Error 'g_id_claim' for index 8\n",
      "Error 'g_id_claim' for index 8\n",
      "Error 'g_id_claim' for index 8\n",
      "Error 'g_id_claim' for index 8\n",
      "Error 'g_id_claim' for index 8\n",
      "Error 'g_id_claim' for index 8\n",
      "Error 'NoneType' object is not subscriptable for index 166\n",
      "Error 'NoneType' object is not subscriptable for index 166\n",
      "Error 'NoneType' object is not subscriptable for index 166\n",
      "Error 'NoneType' object is not subscriptable for index 166\n",
      "Error 'NoneType' object is not subscriptable for index 166\n",
      "Error 'NoneType' object is not subscriptable for index 4492\n",
      "Error 'NoneType' object is not subscriptable for index 4492\n",
      "Error 'NoneType' object is not subscriptable for index 4492\n",
      "Error 'NoneType' object is not subscriptable for index 4492\n",
      "Error 'NoneType' object is not subscriptable for index 4492\n",
      "Error 'NoneType' object is not subscriptable for index 4594\n",
      "Error 'NoneType' object is not subscriptable for index 4594\n",
      "Error 'NoneType' object is not subscriptable for index 4594\n",
      "Error 'NoneType' object is not subscriptable for index 4594\n",
      "Error 'NoneType' object is not subscriptable for index 4594\n",
      "Error 'NoneType' object is not subscriptable for index 4623\n",
      "Error 'NoneType' object is not subscriptable for index 4623\n",
      "Error 'NoneType' object is not subscriptable for index 4623\n",
      "Error 'NoneType' object is not subscriptable for index 4623\n",
      "Error 'NoneType' object is not subscriptable for index 4623\n",
      "Error 'NoneType' object is not subscriptable for index 4699\n",
      "Error 'NoneType' object is not subscriptable for index 4699\n",
      "Error 'NoneType' object is not subscriptable for index 4699\n",
      "Error 'NoneType' object is not subscriptable for index 4699\n",
      "Error 'NoneType' object is not subscriptable for index 4699\n",
      "Error 'NoneType' object is not subscriptable for index 4846\n",
      "Error 'NoneType' object is not subscriptable for index 4846\n",
      "Error 'NoneType' object is not subscriptable for index 4846\n",
      "Error 'NoneType' object is not subscriptable for index 4846\n",
      "Error 'NoneType' object is not subscriptable for index 4846\n",
      "Error 'NoneType' object is not subscriptable for index 5032\n",
      "Error 'NoneType' object is not subscriptable for index 5032\n",
      "Error 'NoneType' object is not subscriptable for index 5032\n",
      "Error 'NoneType' object is not subscriptable for index 5032\n",
      "Error 'NoneType' object is not subscriptable for index 5032\n",
      "Error 'NoneType' object is not subscriptable for index 5143\n",
      "Error 'NoneType' object is not subscriptable for index 5143\n",
      "Error 'NoneType' object is not subscriptable for index 5143\n",
      "Error 'NoneType' object is not subscriptable for index 5143\n",
      "Error 'NoneType' object is not subscriptable for index 5143\n",
      "Error 'NoneType' object is not subscriptable for index 5185\n",
      "Error 'NoneType' object is not subscriptable for index 5185\n",
      "Error 'NoneType' object is not subscriptable for index 5185\n",
      "Error 'NoneType' object is not subscriptable for index 5185\n",
      "Error 'NoneType' object is not subscriptable for index 5185\n",
      "Length of dataset verification: 26550\n"
     ]
    }
   ],
   "source": [
    "# | worker | claim id | label |\n",
    "\n",
    "if task in [\"chart_filtering\"]:\n",
    "    output_df = pd.DataFrame(columns=[\"HIT_id\", \"batch_id\", \"taskSet_id\", \"worker_id\", \"chart_id\", \"chart_img\", \"caption\",\n",
    "                                      \"label\", \"header\", \"events\"])\n",
    "\n",
    "    index = 0\n",
    "    for i, row in annotations_df.iterrows():\n",
    "        for j in range(len(row[\"references\"])):\n",
    "            if row[\"references\"][j][\"g_id\"]!=-1:\n",
    "                continue\n",
    "\n",
    "            output_df.at[index, \"HIT_id\"] = row[\"_id\"]\n",
    "            output_df.at[index, \"batch_id\"] = row[\"batch_id\"]\n",
    "            output_df.at[index, \"taskSet_id\"] = row[\"taskSet_id\"]\n",
    "            output_df.at[index, \"worker_id\"] = row[\"worker_id\"]\n",
    "\n",
    "            output_df.at[index, \"chart_id\"] = row[\"references\"][j][\"db_id\"]\n",
    "            output_df.at[index, \"chart_img\"] = row[\"references\"][j][\"chart_img\"]\n",
    "            output_df.at[index, \"caption\"] = row[\"references\"][j][\"caption\"]\n",
    "\n",
    "            output_df.at[index, \"label\"] = row[\"outputs\"][j][\"label\"]\n",
    "            output_df.at[index, \"header\"] = row[\"outputs\"][j][\"header\"]\n",
    "            output_df.at[index, \"events\"] = row[\"events\"][j]\n",
    "            index += 1\n",
    "\n",
    "    print(len(output_df))\n",
    "    \n",
    "elif task == \"claim_explanation_generation\": \n",
    "    \n",
    "    output_df = pd.DataFrame(columns=[\"HIT_id\", \"batch_id\", \"taskSet_id\", \"worker_id\", \"chart_id\", \"chart_img\", \"caption\",\n",
    "                                      \"claim_text_support\", \"explanation_claim_text_support\", \"claim_text_refute\",\n",
    "                                      \"explanation_claim_text_refute\", \"events\"])\n",
    "    index = 0\n",
    "    for i, row in annotations_df.iterrows():\n",
    "        for j in range(len(row[\"references\"])):\n",
    "            try:\n",
    "                output_df.at[index, \"HIT_id\"] = row[\"_id\"]\n",
    "                output_df.at[index, \"batch_id\"] = row[\"batch_id\"]\n",
    "                output_df.at[index, \"taskSet_id\"] = row[\"taskSet_id\"]\n",
    "                output_df.at[index, \"worker_id\"] = row[\"worker_id\"]\n",
    "\n",
    "                output_df.at[index, \"chart_id\"] = row[\"references\"][j][\"db_id\"]\n",
    "                output_df.at[index, \"chart_img\"] = row[\"references\"][j][\"chart_img\"]\n",
    "                output_df.at[index, \"caption\"] = row[\"references\"][j][\"caption\"]\n",
    "\n",
    "                output_df.at[index, \"claim_text_support\"] = row[\"outputs\"][j][\"claim_text_support\"]\n",
    "                output_df.at[index, \"explanation_claim_text_support\"] = row[\"outputs\"][j][\"explanation_claim_text_support\"]\n",
    "                output_df.at[index, \"claim_text_refute\"] = row[\"outputs\"][j][\"claim_text_refute\"]\n",
    "                output_df.at[index, \"explanation_claim_text_refute\"] = row[\"outputs\"][j][\"explanation_claim_text_refute\"]\n",
    "\n",
    "                output_df.at[index, \"events\"] = row[\"events\"][j]\n",
    "\n",
    "                index += 1  \n",
    "            except Exception: \n",
    "                print(f\"Error for index {i}\")\n",
    "                continue \n",
    "\n",
    "    print(f\"Length of dataset: {len(output_df)}\")\n",
    "    print(f\"Number of claims: {len(output_df)*2}\")\n",
    "    \n",
    "elif task == \"claim_explanation_verification\": \n",
    "    output_df = pd.DataFrame(columns=[\"HIT_id\", \"batch_id\", \"taskSet_id\", \"worker_id\", \"chart_id\", \n",
    "                                      \"chart_img\", \"caption\", \"claim_original\", \"explanation_original\", \n",
    "                                      \"claim_rewritten\", \"explanation_rewritten\", \"label_claim\", \n",
    "                                      \"label_explanation\", \"events\"])\n",
    "    index = 0\n",
    "    for i, row in annotations_df.iterrows():\n",
    "        for j in range(len(row[\"references\"])):\n",
    "            try:\n",
    "                if row[\"references\"][j][\"g_id_claim\"]>-1: \n",
    "                    continue\n",
    "                \n",
    "                output_df.at[index, \"HIT_id\"] = row[\"_id\"]\n",
    "                output_df.at[index, \"batch_id\"] = row[\"batch_id\"]\n",
    "                output_df.at[index, \"taskSet_id\"] = row[\"taskSet_id\"]\n",
    "                output_df.at[index, \"worker_id\"] = row[\"worker_id\"]\n",
    "\n",
    "                output_df.at[index, \"chart_id\"] = row[\"references\"][j][\"db_id\"]\n",
    "                output_df.at[index, \"chart_img\"] = row[\"references\"][j][\"chart_img\"]\n",
    "                output_df.at[index, \"caption\"] = row[\"references\"][j][\"caption\"]\n",
    "\n",
    "                output_df.at[index, \"claim_original\"] = row[\"references\"][j][\"claim\"]\n",
    "                output_df.at[index, \"explanation_original\"] = row[\"references\"][j][\"explanation\"]\n",
    "                \n",
    "                output_df.at[index, \"label\"] = row[\"references\"][j][\"label\"]\n",
    "\n",
    "                output_df.at[index, \"claim_rewritten\"] = row[\"outputs\"][j][\"claim_rewritten\"]\n",
    "                output_df.at[index, \"explanation_rewritten\"] = row[\"outputs\"][j][\"explanation_rewritten\"]\n",
    "                output_df.at[index, \"label_claim\"] = row[\"outputs\"][j][\"label_claim\"]\n",
    "                output_df.at[index, \"label_explanation\"] = row[\"outputs\"][j][\"label_explanation\"]\n",
    "\n",
    "                output_df.at[index, \"events\"] = row[\"events\"][j]\n",
    "                index += 1  \n",
    "                \n",
    "            except Exception as e: \n",
    "                print(f\"Error {e} for index {i}\")\n",
    "                continue \n",
    "\n",
    "    print(f\"Length of dataset verification: {len(output_df)}\")\n",
    "#     print(f\"Number of claims: {len(output_df)*2}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26550"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agreement scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agreement scores: \n",
    "* Krippendorf's alpha: works with nominal, ordinal, and interval data by\n",
    "* Fleiss' kappa: categorical data\n",
    "* Randolph's kappa: also categorical data; BUT to avoid the \"high agreement, low kappa paradox\" [2], Fleiss' kappa is known to be prone to when the true class distribution of the data is unbalanced [1]\n",
    "\n",
    "\n",
    "Other agreement scores: \n",
    "* Scott's π => is equivalent to Fleiss' Kappa but for more than two judges \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reliability_matrix_for_kripp_alpha(df: pd.DataFrame, label_column):\n",
    "    \"\"\"Creates reliability matrix for calculation of Krippendorf's alpha\"\"\"\n",
    "    df = df[['worker_id', 'chart_id', label_column]].groupby(['worker_id', 'chart_id']).agg(np.max).reset_index()\n",
    "    df = df.pivot(index = 'worker_id', columns = 'chart_id', values = label_column).fillna(np.nan)\n",
    "\n",
    "    return df\n",
    "\n",
    "def reliability_matrix_for_kripp_alpha_task_three(df: pd.DataFrame, label_column, index_column):\n",
    "    \"\"\"Creates reliability matrix for calculation of Krippendorf's alpha\"\"\"\n",
    "    df = df[['worker_id', index_column, label_column]].groupby(['worker_id', index_column]).agg(np.max).reset_index()\n",
    "    df = df.pivot(index = 'worker_id', columns = index_column, values = label_column).fillna(np.nan)\n",
    "\n",
    "    return df\n",
    "\n",
    "def reliability_matrix_for_fleiss_kappa(df: pd.DataFrame, label_column):\n",
    "    \"\"\"Creates reliability matrix for calculation of Fleiss kappa\"\"\"\n",
    "    \n",
    "    df = df[['chart_id', label_column]]\n",
    "    df['count'] = 1\n",
    "    df = df.groupby(['chart_id', label_column]).sum().reset_index()\n",
    "    df = df.pivot(index = 'chart_id', columns = label_column, values = 'count').fillna(0)\n",
    "    df = df[df.apply(lambda x : sum(x) == 3.0, axis=1)]\n",
    "\n",
    "    return df\n",
    "\n",
    "def reliability_matrix_for_fleiss_kappa_task_three(df: pd.DataFrame, label_column, index_column):\n",
    "    \"\"\"Creates reliability matrix for calculation of Fleiss kappa\"\"\"\n",
    "    \n",
    "    df = df[[index_column, label_column]]\n",
    "    df['count'] = 1\n",
    "    df = df.groupby([index_column, label_column]).sum().reset_index()\n",
    "    df = df.pivot(index = index_column, columns = label_column, values = 'count').fillna(0)\n",
    "    df = df[df.apply(lambda x : sum(x) == 3.0, axis=1)]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-Kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss' kappa is 0.30348701951423984.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x7/b6tgwcrn1rz_zffbw59v8crr0000gp/T/ipykernel_5093/1838098664.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['count'] = 1\n"
     ]
    }
   ],
   "source": [
    "fleiss_df = reliability_matrix_for_fleiss_kappa(output_df.copy(), label_column=\"label\")\n",
    "\n",
    "fleiss_kappa_val = fleiss_kappa(fleiss_df.values, method = 'fleiss')\n",
    "print('Fleiss\\' kappa is {}.'.format(fleiss_kappa_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 3 (claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss' kappa is 0.3221175382619831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x7/b6tgwcrn1rz_zffbw59v8crr0000gp/T/ipykernel_5093/3233358240.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['count'] = 1\n"
     ]
    }
   ],
   "source": [
    "fleiss_df = reliability_matrix_for_fleiss_kappa_task_three(output_df.copy(), index_column=\"claim_original\",\n",
    "                                                           label_column = \"label_claim\")\n",
    "fleiss_kappa_val = fleiss_kappa(fleiss_df.values, method = 'fleiss')\n",
    "print('Fleiss\\' kappa is {}.'.format(fleiss_kappa_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 3 (explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss' kappa is 0.289801706908303.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x7/b6tgwcrn1rz_zffbw59v8crr0000gp/T/ipykernel_5093/3233358240.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['count'] = 1\n"
     ]
    }
   ],
   "source": [
    "fleiss_df = reliability_matrix_for_fleiss_kappa_task_three(output_df.copy(), index_column=\"explanation_original\",\n",
    "                                                           label_column = \"label_explanation\")\n",
    "fleiss_kappa_val = fleiss_kappa(fleiss_df.values, method = 'fleiss')\n",
    "print('Fleiss\\' kappa is {}.'.format(fleiss_kappa_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-Kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments for Randolph's kappa additionally to Fleiss lappa: \n",
    "* Avoid the high agreement, low kappa paradox [2]\n",
    "* I.e. a high value of observed agreement p, can be drastically lowered by a substantial imbalance of classes in the dataset\n",
    "* Although raters have a high agreement => can result in low Fleiss kappa \n",
    "* Fleiss kappa makes assumptions about the distribution of classes => problematic if imbalance given [2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randolph's kappa is 0.6623767026773133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x7/b6tgwcrn1rz_zffbw59v8crr0000gp/T/ipykernel_5093/1838098664.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['count'] = 1\n"
     ]
    }
   ],
   "source": [
    "randolph_df = reliability_matrix_for_fleiss_kappa(output_df.copy())\n",
    "\n",
    "randolph_kappa_val = fleiss_kappa(randolph_df.values, method = 'randolph')\n",
    "print('Randolph\\'s kappa is {}'.format(randolph_kappa_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 3 (claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randolph's kappa is 0.6145181476846058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x7/b6tgwcrn1rz_zffbw59v8crr0000gp/T/ipykernel_5093/3233358240.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['count'] = 1\n"
     ]
    }
   ],
   "source": [
    "randolph_df = reliability_matrix_for_fleiss_kappa_task_three(output_df.copy(), index_column=\"claim_original\",\n",
    "                                                             label_column = \"label_claim\")\n",
    "randolph_kappa_val = fleiss_kappa(randolph_df.values, method = 'randolph')\n",
    "print('Randolph\\'s kappa is {}'.format(randolph_kappa_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 3 (explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randolph's kappa is 0.5138699408822194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x7/b6tgwcrn1rz_zffbw59v8crr0000gp/T/ipykernel_5093/3233358240.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['count'] = 1\n"
     ]
    }
   ],
   "source": [
    "randolph_df = reliability_matrix_for_fleiss_kappa_task_three(output_df.copy(), index_column=\"explanation_original\",\n",
    "                                                             label_column = \"label_explanation\")\n",
    "randolph_kappa_val = fleiss_kappa(randolph_df.values, method = 'randolph')\n",
    "print('Randolph\\'s kappa is {}'.format(randolph_kappa_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Alpha [4]\n",
    "* Perfect agreement if K-alpha = 1\n",
    "* Alpha = 0 if observed disagreement is equal to disagreement which would result if labels are chosen randomly \n",
    "* K-alpha applicable to: \n",
    " - Any number of observers, not just two\n",
    " - Any number of categories, scale values, or measures\n",
    " - Any metric or level of measurement (nominal, ordinal, interval, ratio, and more)\n",
    " - Incomplete or missing data\n",
    " - Large and small sample sizes alike, not requiring a minimum\n",
    " \n",
    "Arguments for Krippendorf's alpha additionally to Fleiss Kappa: \n",
    "* Can handle missing/incomplete data!\n",
    "* Can handle dataset of different size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's alpha  0.30353196132161164\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>chart_id</th>\n",
       "      <th>6390685299c9ed38aa87098d</th>\n",
       "      <th>6390685299c9ed38aa87098e</th>\n",
       "      <th>6390685299c9ed38aa87098f</th>\n",
       "      <th>6390685299c9ed38aa870990</th>\n",
       "      <th>6390685299c9ed38aa870991</th>\n",
       "      <th>6390685299c9ed38aa870992</th>\n",
       "      <th>6390685299c9ed38aa870993</th>\n",
       "      <th>6390685299c9ed38aa870994</th>\n",
       "      <th>6390685299c9ed38aa870995</th>\n",
       "      <th>6390685299c9ed38aa870996</th>\n",
       "      <th>...</th>\n",
       "      <th>6390687199c9ed38aa871291</th>\n",
       "      <th>6390687199c9ed38aa871292</th>\n",
       "      <th>6390687199c9ed38aa871293</th>\n",
       "      <th>6390687199c9ed38aa871294</th>\n",
       "      <th>6390687199c9ed38aa871295</th>\n",
       "      <th>6390687199c9ed38aa871297</th>\n",
       "      <th>6390687199c9ed38aa871298</th>\n",
       "      <th>6390687199c9ed38aa871299</th>\n",
       "      <th>6390687199c9ed38aa87129a</th>\n",
       "      <th>6390687199c9ed38aa87129b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worker_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A102RSV009OCUW</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A10AJ59UPJFRYL</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A10KXF6PCBWDJN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "chart_id        6390685299c9ed38aa87098d  6390685299c9ed38aa87098e  \\\n",
       "worker_id                                                            \n",
       "A102RSV009OCUW                       NaN                       NaN   \n",
       "A10AJ59UPJFRYL                       NaN                       NaN   \n",
       "A10KXF6PCBWDJN                       NaN                       NaN   \n",
       "\n",
       "chart_id        6390685299c9ed38aa87098f  6390685299c9ed38aa870990  \\\n",
       "worker_id                                                            \n",
       "A102RSV009OCUW                       NaN                       NaN   \n",
       "A10AJ59UPJFRYL                       NaN                       NaN   \n",
       "A10KXF6PCBWDJN                       NaN                       NaN   \n",
       "\n",
       "chart_id        6390685299c9ed38aa870991  6390685299c9ed38aa870992  \\\n",
       "worker_id                                                            \n",
       "A102RSV009OCUW                       NaN                       NaN   \n",
       "A10AJ59UPJFRYL                       NaN                       NaN   \n",
       "A10KXF6PCBWDJN                       NaN                       NaN   \n",
       "\n",
       "chart_id        6390685299c9ed38aa870993  6390685299c9ed38aa870994  \\\n",
       "worker_id                                                            \n",
       "A102RSV009OCUW                       NaN                       NaN   \n",
       "A10AJ59UPJFRYL                       NaN                       NaN   \n",
       "A10KXF6PCBWDJN                       NaN                       NaN   \n",
       "\n",
       "chart_id        6390685299c9ed38aa870995  6390685299c9ed38aa870996  ...  \\\n",
       "worker_id                                                           ...   \n",
       "A102RSV009OCUW                       NaN                       NaN  ...   \n",
       "A10AJ59UPJFRYL                       NaN                       NaN  ...   \n",
       "A10KXF6PCBWDJN                       NaN                       NaN  ...   \n",
       "\n",
       "chart_id        6390687199c9ed38aa871291  6390687199c9ed38aa871292  \\\n",
       "worker_id                                                            \n",
       "A102RSV009OCUW                       NaN                       NaN   \n",
       "A10AJ59UPJFRYL                       NaN                       NaN   \n",
       "A10KXF6PCBWDJN                       NaN                       NaN   \n",
       "\n",
       "chart_id        6390687199c9ed38aa871293  6390687199c9ed38aa871294  \\\n",
       "worker_id                                                            \n",
       "A102RSV009OCUW                       NaN                       NaN   \n",
       "A10AJ59UPJFRYL                       NaN                       NaN   \n",
       "A10KXF6PCBWDJN                       NaN                       NaN   \n",
       "\n",
       "chart_id        6390687199c9ed38aa871295  6390687199c9ed38aa871297  \\\n",
       "worker_id                                                            \n",
       "A102RSV009OCUW                       NaN                       NaN   \n",
       "A10AJ59UPJFRYL                       NaN                       NaN   \n",
       "A10KXF6PCBWDJN                       NaN                       NaN   \n",
       "\n",
       "chart_id        6390687199c9ed38aa871298  6390687199c9ed38aa871299  \\\n",
       "worker_id                                                            \n",
       "A102RSV009OCUW                       NaN                       NaN   \n",
       "A10AJ59UPJFRYL                       NaN                       NaN   \n",
       "A10KXF6PCBWDJN                       NaN                       NaN   \n",
       "\n",
       "chart_id        6390687199c9ed38aa87129a  6390687199c9ed38aa87129b  \n",
       "worker_id                                                           \n",
       "A102RSV009OCUW                       NaN                       NaN  \n",
       "A10AJ59UPJFRYL                       NaN                       NaN  \n",
       "A10KXF6PCBWDJN                       NaN                       NaN  \n",
       "\n",
       "[3 rows x 2132 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kripp_df = reliability_matrix_for_kripp_alpha(output_df.copy())\n",
    "kalpha = krippendorff.alpha(kripp_df.values, level_of_measurement='nominal')\n",
    "print('Krippendorff\\'s alpha  {}'.format(kalpha))\n",
    "\n",
    "kripp_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 3 (claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's alpha  0.32331952981374545\n"
     ]
    }
   ],
   "source": [
    "kripp_df = reliability_matrix_for_kripp_alpha_task_three(output_df.copy(), index_column=\"claim_original\",\n",
    "                                                         label_column = \"label_claim\")\n",
    "kalpha = krippendorff.alpha(kripp_df.values, level_of_measurement='nominal')\n",
    "print('Krippendorff\\'s alpha  {}'.format(kalpha))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 3 (explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's alpha  0.2905870255024222\n"
     ]
    }
   ],
   "source": [
    "kripp_df = reliability_matrix_for_kripp_alpha_task_three(output_df.copy(), index_column=\"explanation_original\",\n",
    "                                                         label_column = \"label_explanation\")\n",
    "kalpha = krippendorff.alpha(kripp_df.values, level_of_measurement='nominal')\n",
    "print('Krippendorff\\'s alpha  {}'.format(kalpha))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation \n",
    "\n",
    "\"Correlation\" measures used should depend on the type of variables being investigated:\n",
    "* continuous variable v continuous variable: use \"traditional\" correlation - e.g. Spearman's rank correlation or Pearson's linear correlation.\n",
    "* continuous variable v categorical variable: use an ANOVA F-test / difference of means\n",
    "* categorical variable v categorical variable: use Chi-square / Cramer's V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation discrete variables\n",
    "* Pair-wise correlation: Pearson's r, Kendall's τ, or Spearman's \\rho \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final dataset\n",
    "\n",
    "dataset = []\n",
    "with jsonlines.open(path_pubhealthtab) as reader:\n",
    "    for line in reader: \n",
    "        dataset.append(line)\n",
    "    \n",
    "print(f\"{len(dataset)} total entries in dataset.\")\n",
    "\n",
    "# convert dataset into pd.DataFrame\n",
    "dataset_df = pd.DataFrame(dataset)\n",
    "dataset_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(columns = [\"claim_len\", \"table_len\"])\n",
    "# corr_df = pd.DataFrame(columns = [\"claim_len\", \"header_rationale_len\", \"table_len\"])\n",
    "\n",
    "corr_df['claim_len'] = [len(nltk.word_tokenize(x)) for x in dataset_df['claim']]\n",
    "# corr_df['header_rationale_len'] = [len(x) for x in dataset_df['header_rationale']]\n",
    "corr_df['table_len'] = [len(x[\"rows\"]) for x in dataset_df['table']]\n",
    "\n",
    "corr_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "\n",
    "correlation_mat = corr_df.corr() # default method = pearson's\n",
    "sns.heatmap(correlation_mat, annot = True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for significance \n",
    "\n",
    "for col in list(corr_df.columns):\n",
    "    p_val = round(pearsonr(corr_df[\"claim_len\"], corr_df[col])[1], 3)\n",
    "    \n",
    "    if p_val < 0.05: \n",
    "        print(f\"The correlation coeff. between 'claim_len' and '{col}' is stat. significant (p-value = {p_val}).\")\n",
    "    else: \n",
    "        print(f\"The correlation coeff. between 'claim_len' and '{col}' is NOT stat. significant (p-value = {p_val}).\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kendall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_mat = corr_df.corr(method=\"kendall\")\n",
    "sns.heatmap(correlation_mat, annot = True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for significance \n",
    "\n",
    "for col in list(corr_df.columns):\n",
    "    p_val = round(kendalltau(corr_df[\"claim_len\"], corr_df[col])[1], 2)\n",
    "    \n",
    "    if p_val < 0.05: \n",
    "        print(f\"The correlation coeff. between 'claim_len' and '{col}' is stat. significant (p-value = {p_val}).\")\n",
    "    else: \n",
    "        print(f\"The correlation coeff. between 'claim_len' and '{col}' is NOT stat. significant (p-value = {p_val}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_mat = corr_df.corr(method=\"spearman\")\n",
    "sns.heatmap(correlation_mat, annot = True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for significance \n",
    "\n",
    "for col in list(corr_df.columns):\n",
    "    p_val = round(spearmanr(corr_df[\"claim_len\"], corr_df[col])[1], 2)\n",
    "    \n",
    "    if p_val < 0.05: \n",
    "        print(f\"The correlation coeff. between 'claim_len' and '{col}' is stat. significant (p-value = {p_val}).\")\n",
    "    else: \n",
    "        print(f\"The correlation coeff. between 'claim_len' and '{col}' is NOT stat. significant (p-value = {p_val}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation categorical variables\n",
    "* Chi-square test (2 categorical variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df['label'] = dataset_df[\"label\"]\n",
    "corr_df['has_table_caption'] = [1 if x[\"caption\"] else 0 for x in dataset_df['table']]\n",
    "corr_df['has_table_header'] = [1 if (x[\"header_horizontal\"] and len(x[\"header_horizontal\"])>0) or (x[\"header_vertical\"] and len(x[\"header_vertical\"])>0) \n",
    "                               else 0 for x in dataset_df['table']]\n",
    "\n",
    "corr_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-square test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label and has_caption\n",
    "\n",
    "cont_table = pd.crosstab(corr_df[\"label\"], corr_df[\"has_table_caption\"]) \n",
    "print(chi2_contingency(cont_table)[1])\n",
    "cont_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label and has_header\n",
    "\n",
    "cont_table = pd.crosstab(corr_df[\"label\"], corr_df[\"has_table_header\"]) \n",
    "print(chi2_contingency(cont_table)[1])\n",
    "cont_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation categorical (e.g. label) and discrete variables\n",
    "\n",
    "* Using __ANOVA F-test__ (1 continuous and 1 categorical variable)\n",
    "\n",
    "* <font color=blue>__Null-hypothesis__</font>: label values (SUPPORTS, REFUTES, NEI) is equally distributed across the 2nd variable, e.g. claim length\n",
    "\n",
    "* If <font color=blue>p-value is less 0.05</font>, we reject the null-hypothesis and can say there is a __stat. significant relation__ between label and 2nd variable [5]\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('claim_len ~ label', data = corr_df).fit()\n",
    "anova_result = sm.stats.anova_lm(model, typ=2)\n",
    "print(f\"P-value is {round(anova_result.iloc[0,3], 3)}\")\n",
    "\n",
    "corr_df[['label', 'claim_len']].boxplot(by='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('table_len ~ label', data = corr_df).fit()\n",
    "anova_result = sm.stats.anova_lm(model, typ=2)\n",
    "print(f\"P-value is {round(anova_result.iloc[0,3], 3)}\")\n",
    "\n",
    "corr_df[['label', 'table_len']].boxplot(by='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('claim_len ~ has_table_caption', data = corr_df).fit()\n",
    "anova_result = sm.stats.anova_lm(model, typ=2)\n",
    "print(f\"P-value is {round(anova_result.iloc[0,3], 3)}\")\n",
    "\n",
    "corr_df[['has_table_caption', 'claim_len']].boxplot(by='has_table_caption')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('claim_len ~ has_table_header', data = corr_df).fit()\n",
    "anova_result = sm.stats.anova_lm(model, typ=2)\n",
    "print(f\"P-value is {round(anova_result.iloc[0,3], 3)}\")\n",
    "\n",
    "corr_df[['has_table_header', 'claim_len']].boxplot(by='has_table_header')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [1] https://files.eric.ed.gov/fulltext/ED490661.pdf\n",
    "    \n",
    "    [2] https://reader.elsevier.com/reader/sd/pii/089543569090158L?token=68830E1F9765B027D7AC8E0260BEF9640E96046B99C8C264BC3222EAB0FD1D41B9C7E24EC24E99C4003168D13B3B48DA&originRegion=eu-west-1&originCreation=20210718072311\n",
    "    \n",
    "    [3] http://up.csail.mit.edu/other-pubs/soylent.pdf\n",
    "    \n",
    "    [4] https://repository.upenn.edu/cgi/viewcontent.cgi?article=1043&context=asc_papers\n",
    "    \n",
    "    [5] https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/anova/how-to/one-way-anova/interpret-the-results/key-results/\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
