{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load python packages\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import jsonlines\n",
    "import krippendorff\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from scipy.stats import chi2_contingency, kendalltau, pearsonr, spearmanr\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.info(\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "This notebook calculates the worker agreement scores and correlations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <strong>Load annotations for a task</strong> \n",
    "\n",
    "2. <strong>Run analysis for agreement among workers</strong> \n",
    "\n",
    "3. <strong>Run analysis for correlations among collected data</strong> \n",
    "\n",
    "\n",
    "Set the following variables first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set one of the following options: \"table_annotation\", \"adjusted_claim_annotation\"\n",
    "task_type = \"\"\n",
    "\n",
    "# Load annotations into pd.Dataframe 'df' \n",
    "df = pd.read_pickle(\"\")\n",
    "\n",
    "# Set path to PubHealthTab dataset (dataset.jsonl) \n",
    "path_pubhealthtab = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df.columns)\n",
    "cols.remove(\"language\")\n",
    "cols.remove(\"type\")\n",
    "cols.remove(\"timestamp\")\n",
    "cols.remove(\"answers\")\n",
    "cols.extend(['worker_id', 'outputs', 'times', 'events', 'feedback'])\n",
    "\n",
    "annotations_df = pd.DataFrame(columns = cols)\n",
    "index = 0\n",
    "\n",
    "for i, row in df.iterrows(): \n",
    "    \n",
    "    for worker_answer in row[\"answers\"]:\n",
    "        row[\"worker_id\"] = worker_answer[\"worker_id\"]\n",
    "        annotations_df.at[index, \"_id\"] = row[\"_id\"]\n",
    "        annotations_df.at[index, \"batch_id\"] = row[\"batch_id\"]\n",
    "        annotations_df.at[index, \"references\"] = row[\"references\"]\n",
    "        annotations_df.at[index, \"taskSet_id\"] = row[\"taskSet_id\"]\n",
    "        annotations_df.at[index, \"hit\"] = row[\"hit\"]\n",
    "        \n",
    "        annotations_df.at[index, \"worker_id\"] = worker_answer[\"worker_id\"]\n",
    "        annotations_df.at[index, \"outputs\"] = worker_answer[\"values\"][\"outputs\"]\n",
    "        annotations_df.at[index, \"times\"] = worker_answer[\"values\"][\"times\"]\n",
    "        annotations_df.at[index, \"events\"] = worker_answer[\"values\"][\"events\"]\n",
    "        annotations_df.at[index, \"feedback\"] = worker_answer[\"values\"][\"feedback\"]\n",
    "        index += 1\n",
    "\n",
    "print(len(annotations_df))\n",
    "annotations_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(columns=[\"HIT_id\", \"batch_id\", \"taskSet_id\", \"worker_id\", \"claim_id\", \"claim\", \"table\",\n",
    "                                      \"label\", \"header\", \"events\"])\n",
    "\n",
    "index = 0\n",
    "for i, row in annotations_df.iterrows():\n",
    "    for j in range(len(row[\"references\"])):\n",
    "        if row[\"references\"][j][\"g_id\"]!=-1:\n",
    "            continue\n",
    "\n",
    "        output_df.at[index, \"HIT_id\"] = row[\"_id\"]\n",
    "        output_df.at[index, \"batch_id\"] = row[\"batch_id\"]\n",
    "        output_df.at[index, \"taskSet_id\"] = row[\"taskSet_id\"]\n",
    "        output_df.at[index, \"worker_id\"] = row[\"worker_id\"]\n",
    "\n",
    "        output_df.at[index, \"claim_id\"] = row[\"references\"][j][\"claim_db_id\"]\n",
    "        output_df.at[index, \"claim\"] = row[\"references\"][j][\"claim\"]\n",
    "        output_df.at[index, \"table\"] = row[\"references\"][j][\"table\"]\n",
    "\n",
    "        output_df.at[index, \"label\"] = row[\"outputs\"][j][\"label\"]\n",
    "        output_df.at[index, \"header\"] = row[\"outputs\"][j][\"header\"]\n",
    "        output_df.at[index, \"events\"] = row[\"events\"][j]\n",
    "        index += 1\n",
    "\n",
    "print(len(output_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agreement scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agreement scores: \n",
    "* Krippendorf's alpha: works with nominal, ordinal, and interval data by\n",
    "* Fleiss' kappa: categorical data\n",
    "* Randolph's kappa: also categorical data; BUT to avoid the \"high agreement, low kappa paradox\" [2], Fleiss' kappa is known to be prone to when the true class distribution of the data is unbalanced [1]\n",
    "\n",
    "\n",
    "Other agreement scores: \n",
    "* Scott's π => is equivalent to Fleiss' Kappa but for more than two judges \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reliability_matrix_for_kripp_alpha(df: pd.DataFrame, three_class_annotation = True):\n",
    "    \"\"\"Creates reliability matrix for calculation of Krippendorf's alpha\"\"\"\n",
    "    if three_class_annotation: \n",
    "        df['label'] = df['label'].apply(lambda x: 2 if x == 3 else x)\n",
    "    \n",
    "    df = df[['worker_id', 'claim_id', 'label']].groupby(['worker_id', 'claim_id']).agg(np.max).reset_index()\n",
    "    df = df.pivot(index = 'worker_id', columns = 'claim_id', values = 'label').fillna(np.nan)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def reliability_matrix_for_fleiss_kappa(df: pd.DataFrame, three_class_annotation = True):\n",
    "    \"\"\"Creates reliability matrix for calculation of Fleiss kappa\"\"\"\n",
    "    if three_class_annotation: \n",
    "        df['label'] = df['label'].apply(lambda x: 2 if x == 3 else x)\n",
    "    \n",
    "    df = df[['claim_id', 'label']]\n",
    "    df['count'] = 1\n",
    "    df = df.groupby(['claim_id', 'label']).sum().reset_index()\n",
    "    df = df.pivot(index = 'claim_id', columns = 'label', values = 'count').fillna(0)\n",
    "    df = df[df.apply(lambda x : sum(x) == 3.0, axis=1)]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-Kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleiss_df = reliability_matrix_for_fleiss_kappa(output_df.copy(), three_class_annotation = False)\n",
    "\n",
    "fleiss_kappa_val = fleiss_kappa(fleiss_df.values, method = 'fleiss')\n",
    "print('Fleiss\\' kappa is {}.'.format(fleiss_kappa_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleiss_df = reliability_matrix_for_fleiss_kappa(output_df.copy(), three_class_annotation = False)\n",
    "\n",
    "fleiss_kappa_val = fleiss_kappa(fleiss_df.values, method = 'fleiss')\n",
    "print('Fleiss\\' kappa is {}.'.format(fleiss_kappa_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-Kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments for Randolph's kappa additionally to Fleiss lappa: \n",
    "* Avoid the high agreement, low kappa paradox [2]\n",
    "* I.e. a high value of observed agreement p, can be drastically lowered by a substantial imbalance of classes in the dataset\n",
    "* Although raters have a high agreement => can result in low Fleiss kappa \n",
    "* Fleiss kappa makes assumptions about the distribution of classes => problematic if imbalance given [2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randolph_df = reliability_matrix_for_fleiss_kappa(output_df.copy(), three_class_annotation = False)\n",
    "\n",
    "randolph_kappa_val = fleiss_kappa(randolph_df.values, method = 'randolph')\n",
    "print('Randolph\\'s kappa is {}'.format(randolph_kappa_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randolph_df = reliability_matrix_for_fleiss_kappa(output_df.copy(), three_class_annotation = False)\n",
    "\n",
    "randolph_kappa_val = fleiss_kappa(randolph_df.values, method = 'randolph')\n",
    "print('Randolph\\'s kappa is {}'.format(randolph_kappa_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Alpha [4]\n",
    "* Perfect agreement if K-alpha = 1\n",
    "* Alpha = 0 if observed disagreement is equal to disagreement which would result if labels are chosen randomly \n",
    "* K-alpha applicable to: \n",
    " - Any number of observers, not just two\n",
    " - Any number of categories, scale values, or measures\n",
    " - Any metric or level of measurement (nominal, ordinal, interval, ratio, and more)\n",
    " - Incomplete or missing data\n",
    " - Large and small sample sizes alike, not requiring a minimum\n",
    " \n",
    "Arguments for Krippendorf's alpha additionally to Fleiss Kappa: \n",
    "* Can handle missing/incomplete data!\n",
    "* Can handle dataset of different size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kripp_df = reliability_matrix_for_kripp_alpha(output_df.copy(), three_class_annotation = False)\n",
    "kalpha = krippendorff.alpha(kripp_df.values, level_of_measurement='nominal')\n",
    "print('Krippendorff\\'s alpha  {}'.format(kalpha))\n",
    "\n",
    "kripp_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kripp_df = reliability_matrix_for_kripp_alpha(output_df.copy(), three_class_annotation = False)\n",
    "kalpha = krippendorff.alpha(kripp_df.values, level_of_measurement='nominal')\n",
    "print('Krippendorff\\'s alpha  {}'.format(kalpha))\n",
    "\n",
    "kripp_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation \n",
    "\n",
    "\"Correlation\" measures used should depend on the type of variables being investigated:\n",
    "* continuous variable v continuous variable: use \"traditional\" correlation - e.g. Spearman's rank correlation or Pearson's linear correlation.\n",
    "* continuous variable v categorical variable: use an ANOVA F-test / difference of means\n",
    "* categorical variable v categorical variable: use Chi-square / Cramer's V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation discrete variables\n",
    "* Pair-wise correlation: Pearson's r, Kendall's τ, or Spearman's \\rho \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final dataset\n",
    "\n",
    "dataset = []\n",
    "with jsonlines.open(path_pubhealthtab) as reader:\n",
    "    for line in reader: \n",
    "        dataset.append(line)\n",
    "    \n",
    "print(f\"{len(dataset)} total entries in dataset.\")\n",
    "\n",
    "# convert dataset into pd.DataFrame\n",
    "dataset_df = pd.DataFrame(dataset)\n",
    "dataset_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(columns = [\"claim_len\", \"table_len\"])\n",
    "# corr_df = pd.DataFrame(columns = [\"claim_len\", \"header_rationale_len\", \"table_len\"])\n",
    "\n",
    "corr_df['claim_len'] = [len(nltk.word_tokenize(x)) for x in dataset_df['claim']]\n",
    "# corr_df['header_rationale_len'] = [len(x) for x in dataset_df['header_rationale']]\n",
    "corr_df['table_len'] = [len(x[\"rows\"]) for x in dataset_df['table']]\n",
    "\n",
    "corr_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "\n",
    "correlation_mat = corr_df.corr() # default method = pearson's\n",
    "sns.heatmap(correlation_mat, annot = True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for significance \n",
    "\n",
    "for col in list(corr_df.columns):\n",
    "    p_val = round(pearsonr(corr_df[\"claim_len\"], corr_df[col])[1], 3)\n",
    "    \n",
    "    if p_val < 0.05: \n",
    "        print(f\"The correlation coeff. between 'claim_len' and '{col}' is stat. significant (p-value = {p_val}).\")\n",
    "    else: \n",
    "        print(f\"The correlation coeff. between 'claim_len' and '{col}' is NOT stat. significant (p-value = {p_val}).\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kendall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_mat = corr_df.corr(method=\"kendall\")\n",
    "sns.heatmap(correlation_mat, annot = True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for significance \n",
    "\n",
    "for col in list(corr_df.columns):\n",
    "    p_val = round(kendalltau(corr_df[\"claim_len\"], corr_df[col])[1], 2)\n",
    "    \n",
    "    if p_val < 0.05: \n",
    "        print(f\"The correlation coeff. between 'claim_len' and '{col}' is stat. significant (p-value = {p_val}).\")\n",
    "    else: \n",
    "        print(f\"The correlation coeff. between 'claim_len' and '{col}' is NOT stat. significant (p-value = {p_val}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_mat = corr_df.corr(method=\"spearman\")\n",
    "sns.heatmap(correlation_mat, annot = True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for significance \n",
    "\n",
    "for col in list(corr_df.columns):\n",
    "    p_val = round(spearmanr(corr_df[\"claim_len\"], corr_df[col])[1], 2)\n",
    "    \n",
    "    if p_val < 0.05: \n",
    "        print(f\"The correlation coeff. between 'claim_len' and '{col}' is stat. significant (p-value = {p_val}).\")\n",
    "    else: \n",
    "        print(f\"The correlation coeff. between 'claim_len' and '{col}' is NOT stat. significant (p-value = {p_val}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation categorical variables\n",
    "* Chi-square test (2 categorical variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df['label'] = dataset_df[\"label\"]\n",
    "corr_df['has_table_caption'] = [1 if x[\"caption\"] else 0 for x in dataset_df['table']]\n",
    "corr_df['has_table_header'] = [1 if (x[\"header_horizontal\"] and len(x[\"header_horizontal\"])>0) or (x[\"header_vertical\"] and len(x[\"header_vertical\"])>0) \n",
    "                               else 0 for x in dataset_df['table']]\n",
    "\n",
    "corr_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-square test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label and has_caption\n",
    "\n",
    "cont_table = pd.crosstab(corr_df[\"label\"], corr_df[\"has_table_caption\"]) \n",
    "print(chi2_contingency(cont_table)[1])\n",
    "cont_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label and has_header\n",
    "\n",
    "cont_table = pd.crosstab(corr_df[\"label\"], corr_df[\"has_table_header\"]) \n",
    "print(chi2_contingency(cont_table)[1])\n",
    "cont_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation categorical (e.g. label) and discrete variables\n",
    "\n",
    "* Using __ANOVA F-test__ (1 continuous and 1 categorical variable)\n",
    "\n",
    "* <font color=blue>__Null-hypothesis__</font>: label values (SUPPORTS, REFUTES, NEI) is equally distributed across the 2nd variable, e.g. claim length\n",
    "\n",
    "* If <font color=blue>p-value is less 0.05</font>, we reject the null-hypothesis and can say there is a __stat. significant relation__ between label and 2nd variable [5]\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('claim_len ~ label', data = corr_df).fit()\n",
    "anova_result = sm.stats.anova_lm(model, typ=2)\n",
    "print(f\"P-value is {round(anova_result.iloc[0,3], 3)}\")\n",
    "\n",
    "corr_df[['label', 'claim_len']].boxplot(by='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('table_len ~ label', data = corr_df).fit()\n",
    "anova_result = sm.stats.anova_lm(model, typ=2)\n",
    "print(f\"P-value is {round(anova_result.iloc[0,3], 3)}\")\n",
    "\n",
    "corr_df[['label', 'table_len']].boxplot(by='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('claim_len ~ has_table_caption', data = corr_df).fit()\n",
    "anova_result = sm.stats.anova_lm(model, typ=2)\n",
    "print(f\"P-value is {round(anova_result.iloc[0,3], 3)}\")\n",
    "\n",
    "corr_df[['has_table_caption', 'claim_len']].boxplot(by='has_table_caption')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('claim_len ~ has_table_header', data = corr_df).fit()\n",
    "anova_result = sm.stats.anova_lm(model, typ=2)\n",
    "print(f\"P-value is {round(anova_result.iloc[0,3], 3)}\")\n",
    "\n",
    "corr_df[['has_table_header', 'claim_len']].boxplot(by='has_table_header')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [1] https://files.eric.ed.gov/fulltext/ED490661.pdf\n",
    "    \n",
    "    [2] https://reader.elsevier.com/reader/sd/pii/089543569090158L?token=68830E1F9765B027D7AC8E0260BEF9640E96046B99C8C264BC3222EAB0FD1D41B9C7E24EC24E99C4003168D13B3B48DA&originRegion=eu-west-1&originCreation=20210718072311\n",
    "    \n",
    "    [3] http://up.csail.mit.edu/other-pubs/soylent.pdf\n",
    "    \n",
    "    [4] https://repository.upenn.edu/cgi/viewcontent.cgi?article=1043&context=asc_papers\n",
    "    \n",
    "    [5] https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/anova/how-to/one-way-anova/interpret-the-results/key-results/\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
